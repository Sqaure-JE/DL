{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>car_01_cat</th>\n",
       "      <th>car_02_cat</th>\n",
       "      <th>car_03_cat</th>\n",
       "      <th>car_04_cat</th>\n",
       "      <th>car_05</th>\n",
       "      <th>car_06</th>\n",
       "      <th>car_07_cat</th>\n",
       "      <th>car_08_cat</th>\n",
       "      <th>...</th>\n",
       "      <th>idx_14</th>\n",
       "      <th>idx_15_bin</th>\n",
       "      <th>idx_16_bin</th>\n",
       "      <th>idx_17</th>\n",
       "      <th>idx_18</th>\n",
       "      <th>idx_19</th>\n",
       "      <th>idx_20</th>\n",
       "      <th>loc_01</th>\n",
       "      <th>loc_02</th>\n",
       "      <th>loc_03</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C000000</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>3.316625</td>\n",
       "      <td>0.370810</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.878564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C000001</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>2.449490</td>\n",
       "      <td>0.374433</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.981708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C000002</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>3.464102</td>\n",
       "      <td>0.423084</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.495606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C000003</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>93</td>\n",
       "      <td>3.741657</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.9</td>\n",
       "      <td>2.066549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C000004</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>104</td>\n",
       "      <td>3.741657</td>\n",
       "      <td>0.430116</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.740355</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  target  car_01_cat  car_02_cat  car_03_cat  car_04_cat    car_05  \\\n",
       "0  C000000       0           4          -1           0          12  3.316625   \n",
       "1  C000001       0           0           1           0          39  2.449490   \n",
       "2  C000002       0           0          -1           2         100  3.464102   \n",
       "3  C000003       0           9           1           0          93  3.741657   \n",
       "4  C000004       0           0           0           1         104  3.741657   \n",
       "\n",
       "     car_06  car_07_cat  car_08_cat  ...  idx_14  idx_15_bin  idx_16_bin  \\\n",
       "0  0.370810           1           1  ...       7           0           0   \n",
       "1  0.374433           1           1  ...      11           0           1   \n",
       "2  0.423084           1           1  ...      11           0           0   \n",
       "3  0.316228           0           0  ...       7           0           0   \n",
       "4  0.430116           1           0  ...       6           0           0   \n",
       "\n",
       "   idx_17  idx_18  idx_19  idx_20  loc_01  loc_02    loc_03  \n",
       "0       0       3       7       1     0.6     0.7  0.878564  \n",
       "1       0       6       8       1     0.6     0.6  0.981708  \n",
       "2       0       1       8       3     0.2     0.9  0.495606  \n",
       "3       3       3       8       4     1.8     0.9  2.066549  \n",
       "4       1       1       7       4     0.0     0.4  0.740355  \n",
       "\n",
       "[5 rows x 59 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>car_01_cat</th>\n",
       "      <th>car_02_cat</th>\n",
       "      <th>car_03_cat</th>\n",
       "      <th>car_04_cat</th>\n",
       "      <th>car_05</th>\n",
       "      <th>car_06</th>\n",
       "      <th>car_07_cat</th>\n",
       "      <th>car_08_cat</th>\n",
       "      <th>car_09</th>\n",
       "      <th>...</th>\n",
       "      <th>idx_14</th>\n",
       "      <th>idx_15_bin</th>\n",
       "      <th>idx_16_bin</th>\n",
       "      <th>idx_17</th>\n",
       "      <th>idx_18</th>\n",
       "      <th>idx_19</th>\n",
       "      <th>idx_20</th>\n",
       "      <th>loc_01</th>\n",
       "      <th>loc_02</th>\n",
       "      <th>loc_03</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>3.316625</td>\n",
       "      <td>0.370810</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.754792</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.878564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>2.449490</td>\n",
       "      <td>0.374433</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.614780</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.981708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>3.464102</td>\n",
       "      <td>0.423084</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.815665</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.495606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>93</td>\n",
       "      <td>3.741657</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.068114</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.9</td>\n",
       "      <td>2.066549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>104</td>\n",
       "      <td>3.741657</td>\n",
       "      <td>0.430116</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.509768</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.740355</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   target  car_01_cat  car_02_cat  car_03_cat  car_04_cat    car_05    car_06  \\\n",
       "0       0           4          -1           0          12  3.316625  0.370810   \n",
       "1       0           0           1           0          39  2.449490  0.374433   \n",
       "2       0           0          -1           2         100  3.464102  0.423084   \n",
       "3       0           9           1           0          93  3.741657  0.316228   \n",
       "4       0           0           0           1         104  3.741657  0.430116   \n",
       "\n",
       "   car_07_cat  car_08_cat    car_09  ...  idx_14  idx_15_bin  idx_16_bin  \\\n",
       "0           1           1  0.754792  ...       7           0           0   \n",
       "1           1           1  0.614780  ...      11           0           1   \n",
       "2           1           1  0.815665  ...      11           0           0   \n",
       "3           0           0  1.068114  ...       7           0           0   \n",
       "4           1           0  1.509768  ...       6           0           0   \n",
       "\n",
       "   idx_17  idx_18  idx_19  idx_20  loc_01  loc_02    loc_03  \n",
       "0       0       3       7       1     0.6     0.7  0.878564  \n",
       "1       0       6       8       1     0.6     0.6  0.981708  \n",
       "2       0       1       8       3     0.2     0.9  0.495606  \n",
       "3       3       3       8       4     1.8     0.9  2.066549  \n",
       "4       1       1       7       4     0.0     0.4  0.740355  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop('id', 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.target\n",
    "df = df.drop('target',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>car_01_cat</th>\n",
       "      <td>416712.0</td>\n",
       "      <td>0.385634</td>\n",
       "      <td>0.323565</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>car_02_cat</th>\n",
       "      <td>416712.0</td>\n",
       "      <td>0.420812</td>\n",
       "      <td>0.422253</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>car_03_cat</th>\n",
       "      <td>416712.0</td>\n",
       "      <td>0.080254</td>\n",
       "      <td>0.238822</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>car_04_cat</th>\n",
       "      <td>416712.0</td>\n",
       "      <td>0.594044</td>\n",
       "      <td>0.320645</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.300971</td>\n",
       "      <td>0.621359</td>\n",
       "      <td>0.893204</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>car_05</th>\n",
       "      <td>416712.0</td>\n",
       "      <td>0.819350</td>\n",
       "      <td>0.195494</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.755929</td>\n",
       "      <td>0.886405</td>\n",
       "      <td>0.963624</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>car_06</th>\n",
       "      <td>416712.0</td>\n",
       "      <td>0.780102</td>\n",
       "      <td>0.217940</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.814697</td>\n",
       "      <td>0.836461</td>\n",
       "      <td>0.853621</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>car_07_cat</th>\n",
       "      <td>416712.0</td>\n",
       "      <td>0.914824</td>\n",
       "      <td>0.187984</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>car_08_cat</th>\n",
       "      <td>416712.0</td>\n",
       "      <td>0.832371</td>\n",
       "      <td>0.373537</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>car_09</th>\n",
       "      <td>416712.0</td>\n",
       "      <td>0.162101</td>\n",
       "      <td>0.064681</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.121109</td>\n",
       "      <td>0.148451</td>\n",
       "      <td>0.188862</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>car_10</th>\n",
       "      <td>416712.0</td>\n",
       "      <td>0.836686</td>\n",
       "      <td>0.208023</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>car_11</th>\n",
       "      <td>416712.0</td>\n",
       "      <td>0.609255</td>\n",
       "      <td>0.025719</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.581139</td>\n",
       "      <td>0.606720</td>\n",
       "      <td>0.618126</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>car_12_cat</th>\n",
       "      <td>416712.0</td>\n",
       "      <td>0.465696</td>\n",
       "      <td>0.195767</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>car_13_cat</th>\n",
       "      <td>416712.0</td>\n",
       "      <td>0.774333</td>\n",
       "      <td>0.209172</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>car_14_cat</th>\n",
       "      <td>416712.0</td>\n",
       "      <td>0.247070</td>\n",
       "      <td>0.394124</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>car_15_cat</th>\n",
       "      <td>416712.0</td>\n",
       "      <td>0.496073</td>\n",
       "      <td>0.045766</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>car_16_cat</th>\n",
       "      <td>416712.0</td>\n",
       "      <td>0.955034</td>\n",
       "      <td>0.173534</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cus_01_bin</th>\n",
       "      <td>416712.0</td>\n",
       "      <td>0.394073</td>\n",
       "      <td>0.488651</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cus_02</th>\n",
       "      <td>416712.0</td>\n",
       "      <td>0.402284</td>\n",
       "      <td>0.245615</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cus_03_bin</th>\n",
       "      <td>416712.0</td>\n",
       "      <td>0.009455</td>\n",
       "      <td>0.096776</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cus_04_bin</th>\n",
       "      <td>416712.0</td>\n",
       "      <td>0.000372</td>\n",
       "      <td>0.019283</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cus_05</th>\n",
       "      <td>416712.0</td>\n",
       "      <td>0.003126</td>\n",
       "      <td>0.032062</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cus_06_bin</th>\n",
       "      <td>416712.0</td>\n",
       "      <td>0.152854</td>\n",
       "      <td>0.359847</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cus_07_bin</th>\n",
       "      <td>416712.0</td>\n",
       "      <td>0.185476</td>\n",
       "      <td>0.388684</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cus_08_bin</th>\n",
       "      <td>416712.0</td>\n",
       "      <td>0.164133</td>\n",
       "      <td>0.370396</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cus_09</th>\n",
       "      <td>416712.0</td>\n",
       "      <td>0.561686</td>\n",
       "      <td>0.272711</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cus_10_cat</th>\n",
       "      <td>416712.0</td>\n",
       "      <td>0.708258</td>\n",
       "      <td>0.246637</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cus_11_bin</th>\n",
       "      <td>416712.0</td>\n",
       "      <td>0.121019</td>\n",
       "      <td>0.326150</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cus_12_bin</th>\n",
       "      <td>416712.0</td>\n",
       "      <td>0.001745</td>\n",
       "      <td>0.041732</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cus_13</th>\n",
       "      <td>416712.0</td>\n",
       "      <td>0.271541</td>\n",
       "      <td>0.283442</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cus_14_cat</th>\n",
       "      <td>416712.0</td>\n",
       "      <td>0.200775</td>\n",
       "      <td>0.193067</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cus_15_bin</th>\n",
       "      <td>416712.0</td>\n",
       "      <td>0.661354</td>\n",
       "      <td>0.473250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cus_16_cat</th>\n",
       "      <td>416712.0</td>\n",
       "      <td>0.471746</td>\n",
       "      <td>0.133021</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cus_17_bin</th>\n",
       "      <td>416712.0</td>\n",
       "      <td>0.000933</td>\n",
       "      <td>0.030539</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cus_18_bin</th>\n",
       "      <td>416712.0</td>\n",
       "      <td>0.256319</td>\n",
       "      <td>0.436600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idx_01_bin</th>\n",
       "      <td>416712.0</td>\n",
       "      <td>0.554584</td>\n",
       "      <td>0.497012</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idx_02</th>\n",
       "      <td>416712.0</td>\n",
       "      <td>0.314208</td>\n",
       "      <td>0.189247</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idx_03</th>\n",
       "      <td>416712.0</td>\n",
       "      <td>0.499604</td>\n",
       "      <td>0.318834</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idx_04</th>\n",
       "      <td>416712.0</td>\n",
       "      <td>0.499611</td>\n",
       "      <td>0.319141</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idx_05_bin</th>\n",
       "      <td>416712.0</td>\n",
       "      <td>0.628043</td>\n",
       "      <td>0.483328</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idx_06_bin</th>\n",
       "      <td>416712.0</td>\n",
       "      <td>0.122643</td>\n",
       "      <td>0.328028</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idx_07</th>\n",
       "      <td>416712.0</td>\n",
       "      <td>0.334277</td>\n",
       "      <td>0.178275</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idx_08</th>\n",
       "      <td>416712.0</td>\n",
       "      <td>0.342795</td>\n",
       "      <td>0.124942</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idx_09</th>\n",
       "      <td>416712.0</td>\n",
       "      <td>0.474453</td>\n",
       "      <td>0.223442</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idx_10</th>\n",
       "      <td>416712.0</td>\n",
       "      <td>0.500297</td>\n",
       "      <td>0.318962</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idx_11</th>\n",
       "      <td>416712.0</td>\n",
       "      <td>0.302217</td>\n",
       "      <td>0.129606</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idx_12</th>\n",
       "      <td>416712.0</td>\n",
       "      <td>0.769034</td>\n",
       "      <td>0.133395</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idx_13_bin</th>\n",
       "      <td>416712.0</td>\n",
       "      <td>0.287064</td>\n",
       "      <td>0.452392</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idx_14</th>\n",
       "      <td>416712.0</td>\n",
       "      <td>0.722549</td>\n",
       "      <td>0.145990</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idx_15_bin</th>\n",
       "      <td>416712.0</td>\n",
       "      <td>0.349068</td>\n",
       "      <td>0.476676</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idx_16_bin</th>\n",
       "      <td>416712.0</td>\n",
       "      <td>0.153262</td>\n",
       "      <td>0.360240</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idx_17</th>\n",
       "      <td>416712.0</td>\n",
       "      <td>0.143908</td>\n",
       "      <td>0.120096</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idx_18</th>\n",
       "      <td>416712.0</td>\n",
       "      <td>0.220995</td>\n",
       "      <td>0.130445</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idx_19</th>\n",
       "      <td>416712.0</td>\n",
       "      <td>0.351385</td>\n",
       "      <td>0.121024</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idx_20</th>\n",
       "      <td>416712.0</td>\n",
       "      <td>0.333839</td>\n",
       "      <td>0.157197</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loc_01</th>\n",
       "      <td>416712.0</td>\n",
       "      <td>0.244155</td>\n",
       "      <td>0.224658</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loc_02</th>\n",
       "      <td>416712.0</td>\n",
       "      <td>0.679179</td>\n",
       "      <td>0.319688</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loc_03</th>\n",
       "      <td>416712.0</td>\n",
       "      <td>0.307809</td>\n",
       "      <td>0.157520</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.302585</td>\n",
       "      <td>0.341543</td>\n",
       "      <td>0.396987</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               count      mean       std  min       25%       50%       75%  \\\n",
       "car_01_cat  416712.0  0.385634  0.323565  0.0  0.058824  0.411765  0.647059   \n",
       "car_02_cat  416712.0  0.420812  0.422253  0.0  0.000000  0.500000  1.000000   \n",
       "car_03_cat  416712.0  0.080254  0.238822  0.0  0.000000  0.000000  0.000000   \n",
       "car_04_cat  416712.0  0.594044  0.320645  0.0  0.300971  0.621359  0.893204   \n",
       "car_05      416712.0  0.819350  0.195494  0.0  0.755929  0.886405  0.963624   \n",
       "car_06      416712.0  0.780102  0.217940  0.0  0.814697  0.836461  0.853621   \n",
       "car_07_cat  416712.0  0.914824  0.187984  0.0  1.000000  1.000000  1.000000   \n",
       "car_08_cat  416712.0  0.832371  0.373537  0.0  1.000000  1.000000  1.000000   \n",
       "car_09      416712.0  0.162101  0.064681  0.0  0.121109  0.148451  0.188862   \n",
       "car_10      416712.0  0.836686  0.208023  0.0  0.750000  1.000000  1.000000   \n",
       "car_11      416712.0  0.609255  0.025719  0.0  0.581139  0.606720  0.618126   \n",
       "car_12_cat  416712.0  0.465696  0.195767  0.0  0.200000  0.600000  0.600000   \n",
       "car_13_cat  416712.0  0.774333  0.209172  0.0  0.666667  0.666667  1.000000   \n",
       "car_14_cat  416712.0  0.247070  0.394124  0.0  0.000000  0.000000  0.500000   \n",
       "car_15_cat  416712.0  0.496073  0.045766  0.0  0.500000  0.500000  0.500000   \n",
       "car_16_cat  416712.0  0.955034  0.173534  0.0  1.000000  1.000000  1.000000   \n",
       "cus_01_bin  416712.0  0.394073  0.488651  0.0  0.000000  0.000000  1.000000   \n",
       "cus_02      416712.0  0.402284  0.245615  0.0  0.181818  0.363636  0.545455   \n",
       "cus_03_bin  416712.0  0.009455  0.096776  0.0  0.000000  0.000000  0.000000   \n",
       "cus_04_bin  416712.0  0.000372  0.019283  0.0  0.000000  0.000000  0.000000   \n",
       "cus_05      416712.0  0.003126  0.032062  0.0  0.000000  0.000000  0.000000   \n",
       "cus_06_bin  416712.0  0.152854  0.359847  0.0  0.000000  0.000000  0.000000   \n",
       "cus_07_bin  416712.0  0.185476  0.388684  0.0  0.000000  0.000000  0.000000   \n",
       "cus_08_bin  416712.0  0.164133  0.370396  0.0  0.000000  0.000000  0.000000   \n",
       "cus_09      416712.0  0.561686  0.272711  0.0  0.384615  0.538462  0.769231   \n",
       "cus_10_cat  416712.0  0.708258  0.246637  0.0  0.500000  0.500000  1.000000   \n",
       "cus_11_bin  416712.0  0.121019  0.326150  0.0  0.000000  0.000000  0.000000   \n",
       "cus_12_bin  416712.0  0.001745  0.041732  0.0  0.000000  0.000000  0.000000   \n",
       "cus_13      416712.0  0.271541  0.283442  0.0  0.000000  0.142857  0.428571   \n",
       "cus_14_cat  416712.0  0.200775  0.193067  0.0  0.142857  0.142857  0.142857   \n",
       "cus_15_bin  416712.0  0.661354  0.473250  0.0  0.000000  1.000000  1.000000   \n",
       "cus_16_cat  416712.0  0.471746  0.133021  0.0  0.400000  0.400000  0.600000   \n",
       "cus_17_bin  416712.0  0.000933  0.030539  0.0  0.000000  0.000000  0.000000   \n",
       "cus_18_bin  416712.0  0.256319  0.436600  0.0  0.000000  0.000000  1.000000   \n",
       "idx_01_bin  416712.0  0.554584  0.497012  0.0  0.000000  1.000000  1.000000   \n",
       "idx_02      416712.0  0.314208  0.189247  0.0  0.166667  0.333333  0.500000   \n",
       "idx_03      416712.0  0.499604  0.318834  0.0  0.222222  0.444444  0.777778   \n",
       "idx_04      416712.0  0.499611  0.319141  0.0  0.222222  0.555556  0.777778   \n",
       "idx_05_bin  416712.0  0.628043  0.483328  0.0  0.000000  1.000000  1.000000   \n",
       "idx_06_bin  416712.0  0.122643  0.328028  0.0  0.000000  0.000000  0.000000   \n",
       "idx_07      416712.0  0.334277  0.178275  0.0  0.142857  0.285714  0.428571   \n",
       "idx_08      416712.0  0.342795  0.124942  0.0  0.272727  0.318182  0.409091   \n",
       "idx_09      416712.0  0.474453  0.223442  0.0  0.400000  0.400000  0.600000   \n",
       "idx_10      416712.0  0.500297  0.318962  0.0  0.222222  0.555556  0.777778   \n",
       "idx_11      416712.0  0.302217  0.129606  0.0  0.222222  0.277778  0.388889   \n",
       "idx_12      416712.0  0.769034  0.133395  0.0  0.700000  0.800000  0.900000   \n",
       "idx_13_bin  416712.0  0.287064  0.452392  0.0  0.000000  0.000000  1.000000   \n",
       "idx_14      416712.0  0.722549  0.145990  0.0  0.600000  0.700000  0.800000   \n",
       "idx_15_bin  416712.0  0.349068  0.476676  0.0  0.000000  0.000000  1.000000   \n",
       "idx_16_bin  416712.0  0.153262  0.360240  0.0  0.000000  0.000000  0.000000   \n",
       "idx_17      416712.0  0.143908  0.120096  0.0  0.100000  0.100000  0.200000   \n",
       "idx_18      416712.0  0.220995  0.130445  0.0  0.153846  0.230769  0.307692   \n",
       "idx_19      416712.0  0.351385  0.121024  0.0  0.250000  0.333333  0.416667   \n",
       "idx_20      416712.0  0.333839  0.157197  0.0  0.222222  0.333333  0.444444   \n",
       "loc_01      416712.0  0.244155  0.224658  0.0  0.111111  0.166667  0.333333   \n",
       "loc_02      416712.0  0.679179  0.319688  0.0  0.444444  0.777778  1.000000   \n",
       "loc_03      416712.0  0.307809  0.157520  0.0  0.302585  0.341543  0.396987   \n",
       "\n",
       "            max  \n",
       "car_01_cat  1.0  \n",
       "car_02_cat  1.0  \n",
       "car_03_cat  1.0  \n",
       "car_04_cat  1.0  \n",
       "car_05      1.0  \n",
       "car_06      1.0  \n",
       "car_07_cat  1.0  \n",
       "car_08_cat  1.0  \n",
       "car_09      1.0  \n",
       "car_10      1.0  \n",
       "car_11      1.0  \n",
       "car_12_cat  1.0  \n",
       "car_13_cat  1.0  \n",
       "car_14_cat  1.0  \n",
       "car_15_cat  1.0  \n",
       "car_16_cat  1.0  \n",
       "cus_01_bin  1.0  \n",
       "cus_02      1.0  \n",
       "cus_03_bin  1.0  \n",
       "cus_04_bin  1.0  \n",
       "cus_05      1.0  \n",
       "cus_06_bin  1.0  \n",
       "cus_07_bin  1.0  \n",
       "cus_08_bin  1.0  \n",
       "cus_09      1.0  \n",
       "cus_10_cat  1.0  \n",
       "cus_11_bin  1.0  \n",
       "cus_12_bin  1.0  \n",
       "cus_13      1.0  \n",
       "cus_14_cat  1.0  \n",
       "cus_15_bin  1.0  \n",
       "cus_16_cat  1.0  \n",
       "cus_17_bin  1.0  \n",
       "cus_18_bin  1.0  \n",
       "idx_01_bin  1.0  \n",
       "idx_02      1.0  \n",
       "idx_03      1.0  \n",
       "idx_04      1.0  \n",
       "idx_05_bin  1.0  \n",
       "idx_06_bin  1.0  \n",
       "idx_07      1.0  \n",
       "idx_08      1.0  \n",
       "idx_09      1.0  \n",
       "idx_10      1.0  \n",
       "idx_11      1.0  \n",
       "idx_12      1.0  \n",
       "idx_13_bin  1.0  \n",
       "idx_14      1.0  \n",
       "idx_15_bin  1.0  \n",
       "idx_16_bin  1.0  \n",
       "idx_17      1.0  \n",
       "idx_18      1.0  \n",
       "idx_19      1.0  \n",
       "idx_20      1.0  \n",
       "loc_01      1.0  \n",
       "loc_02      1.0  \n",
       "loc_03      1.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "df = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(df, y, test_size=0.3, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.optimizers import SGD\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 128)               7424      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 18,473\n",
      "Trainable params: 18,473\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "#keras.layers.normalization.BatchNormalization(epsilon=1e-06, momentum=0.9, axis=-1)\n",
    "model.add(Dense(128, activation='elu', input_shape=(df.shape[1],)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='elu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(32, activation='elu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(16, activation='elu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(8, activation='elu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(4, activation='elu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2, activation='elu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "#model.compile(optimizer=SGD(learning_rate=0.0005, momentum=0.9 ), loss='binary_crossentropy', metrics=['acc', tf.keras.metrics.AUC(name='auc')])\n",
    "model.compile(optimizer=Adam(learning_rate=0.0002,decay = 1e-6 ), loss='binary_crossentropy', metrics=['acc', tf.keras.metrics.AUC(name='auc')])\n",
    "#model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc', tf.keras.metrics.AUC(name='auc')])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(patience=50, verbose=1)\n",
    "mc= ModelCheckpoint('best.h5', save_best_only= True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 291698 samples, validate on 125014 samples\n",
      "Epoch 1/1000\n",
      "291698/291698 [==============================] - 35s 120us/step - loss: 0.4008 - acc: 0.9126 - auc: 0.4986 - val_loss: 0.1573 - val_acc: 0.9635 - val_auc: 0.4981\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.15728, saving model to best.h5\n",
      "Epoch 2/1000\n",
      "291698/291698 [==============================] - 34s 118us/step - loss: 0.2700 - acc: 0.9631 - auc: 0.4998 - val_loss: 0.1585 - val_acc: 0.9635 - val_auc: 0.5010\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.15728\n",
      "Epoch 3/1000\n",
      "291698/291698 [==============================] - 34s 118us/step - loss: 0.2258 - acc: 0.9634 - auc: 0.5017 - val_loss: 0.1595 - val_acc: 0.9635 - val_auc: 0.5023\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.15728\n",
      "Epoch 4/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1975 - acc: 0.9635 - auc: 0.5040 - val_loss: 0.1582 - val_acc: 0.9635 - val_auc: 0.5065\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.15728\n",
      "Epoch 5/1000\n",
      "291698/291698 [==============================] - 35s 118us/step - loss: 0.1818 - acc: 0.9635 - auc: 0.5084 - val_loss: 0.1578 - val_acc: 0.9635 - val_auc: 0.5101\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.15728\n",
      "Epoch 6/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1713 - acc: 0.9635 - auc: 0.5117 - val_loss: 0.1564 - val_acc: 0.9635 - val_auc: 0.5132\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.15728 to 0.15635, saving model to best.h5\n",
      "Epoch 7/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1644 - acc: 0.9635 - auc: 0.5148 - val_loss: 0.1555 - val_acc: 0.9635 - val_auc: 0.5158\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.15635 to 0.15547, saving model to best.h5\n",
      "Epoch 8/1000\n",
      "291698/291698 [==============================] - 34s 118us/step - loss: 0.1605 - acc: 0.9635 - auc: 0.5168 - val_loss: 0.1552 - val_acc: 0.9635 - val_auc: 0.5176\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.15547 to 0.15515, saving model to best.h5\n",
      "Epoch 9/1000\n",
      "291698/291698 [==============================] - 35s 118us/step - loss: 0.1584 - acc: 0.9635 - auc: 0.5186 - val_loss: 0.1550 - val_acc: 0.9635 - val_auc: 0.5192\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.15515 to 0.15499, saving model to best.h5\n",
      "Epoch 10/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1573 - acc: 0.9635 - auc: 0.5200 - val_loss: 0.1549 - val_acc: 0.9635 - val_auc: 0.5211\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.15499 to 0.15487, saving model to best.h5\n",
      "Epoch 11/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1565 - acc: 0.9635 - auc: 0.5222 - val_loss: 0.1545 - val_acc: 0.9635 - val_auc: 0.5233\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.15487 to 0.15446, saving model to best.h5\n",
      "Epoch 12/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1559 - acc: 0.9635 - auc: 0.5246 - val_loss: 0.1546 - val_acc: 0.9635 - val_auc: 0.5257\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.15446\n",
      "Epoch 13/1000\n",
      "291698/291698 [==============================] - 35s 118us/step - loss: 0.1558 - acc: 0.9635 - auc: 0.5268 - val_loss: 0.1545 - val_acc: 0.9635 - val_auc: 0.5281\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.15446 to 0.15446, saving model to best.h5\n",
      "Epoch 14/1000\n",
      "291698/291698 [==============================] - 35s 118us/step - loss: 0.1557 - acc: 0.9635 - auc: 0.5291 - val_loss: 0.1545 - val_acc: 0.9635 - val_auc: 0.5301\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.15446\n",
      "Epoch 15/1000\n",
      "291698/291698 [==============================] - 34s 118us/step - loss: 0.1555 - acc: 0.9635 - auc: 0.5312 - val_loss: 0.1544 - val_acc: 0.9635 - val_auc: 0.5323\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.15446 to 0.15444, saving model to best.h5\n",
      "Epoch 16/1000\n",
      "291698/291698 [==============================] - 35s 118us/step - loss: 0.1555 - acc: 0.9635 - auc: 0.5333 - val_loss: 0.1543 - val_acc: 0.9635 - val_auc: 0.5344\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.15444 to 0.15426, saving model to best.h5\n",
      "Epoch 17/1000\n",
      "291698/291698 [==============================] - 34s 118us/step - loss: 0.1553 - acc: 0.9635 - auc: 0.5356 - val_loss: 0.1545 - val_acc: 0.9635 - val_auc: 0.5365\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.15426\n",
      "Epoch 18/1000\n",
      "291698/291698 [==============================] - 35s 118us/step - loss: 0.1556 - acc: 0.9635 - auc: 0.5373 - val_loss: 0.1546 - val_acc: 0.9635 - val_auc: 0.5381\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.15426\n",
      "Epoch 19/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1554 - acc: 0.9635 - auc: 0.5389 - val_loss: 0.1545 - val_acc: 0.9635 - val_auc: 0.5397\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.15426\n",
      "Epoch 20/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1553 - acc: 0.9635 - auc: 0.5406 - val_loss: 0.1542 - val_acc: 0.9635 - val_auc: 0.5415\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.15426 to 0.15421, saving model to best.h5\n",
      "Epoch 21/1000\n",
      "291698/291698 [==============================] - 34s 118us/step - loss: 0.1553 - acc: 0.9635 - auc: 0.5423 - val_loss: 0.1543 - val_acc: 0.9635 - val_auc: 0.5431\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.15421\n",
      "Epoch 22/1000\n",
      "291698/291698 [==============================] - 34s 118us/step - loss: 0.1554 - acc: 0.9635 - auc: 0.5439 - val_loss: 0.1544 - val_acc: 0.9635 - val_auc: 0.5446\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.15421\n",
      "Epoch 23/1000\n",
      "291698/291698 [==============================] - 35s 118us/step - loss: 0.1553 - acc: 0.9635 - auc: 0.5452 - val_loss: 0.1543 - val_acc: 0.9635 - val_auc: 0.5460\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.15421\n",
      "Epoch 24/1000\n",
      "291698/291698 [==============================] - 35s 118us/step - loss: 0.1553 - acc: 0.9635 - auc: 0.5468 - val_loss: 0.1542 - val_acc: 0.9635 - val_auc: 0.5475\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.15421 to 0.15418, saving model to best.h5\n",
      "Epoch 25/1000\n",
      "291698/291698 [==============================] - 35s 118us/step - loss: 0.1552 - acc: 0.9635 - auc: 0.5482 - val_loss: 0.1544 - val_acc: 0.9635 - val_auc: 0.5488\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.15418\n",
      "Epoch 26/1000\n",
      "291698/291698 [==============================] - 35s 118us/step - loss: 0.1553 - acc: 0.9635 - auc: 0.5494 - val_loss: 0.1542 - val_acc: 0.9635 - val_auc: 0.5500\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.15418 to 0.15417, saving model to best.h5\n",
      "Epoch 27/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1553 - acc: 0.9635 - auc: 0.5507 - val_loss: 0.1543 - val_acc: 0.9635 - val_auc: 0.5512\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.15417\n",
      "Epoch 28/1000\n",
      "291698/291698 [==============================] - 35s 118us/step - loss: 0.1552 - acc: 0.9635 - auc: 0.5517 - val_loss: 0.1542 - val_acc: 0.9635 - val_auc: 0.5523\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.15417 to 0.15415, saving model to best.h5\n",
      "Epoch 29/1000\n",
      "291698/291698 [==============================] - 34s 118us/step - loss: 0.1553 - acc: 0.9635 - auc: 0.5528 - val_loss: 0.1544 - val_acc: 0.9635 - val_auc: 0.5533\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.15415\n",
      "Epoch 30/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1552 - acc: 0.9635 - auc: 0.5538 - val_loss: 0.1544 - val_acc: 0.9635 - val_auc: 0.5542\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.15415\n",
      "Epoch 31/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1551 - acc: 0.9635 - auc: 0.5547 - val_loss: 0.1541 - val_acc: 0.9635 - val_auc: 0.5552\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.15415 to 0.15414, saving model to best.h5\n",
      "Epoch 32/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1552 - acc: 0.9635 - auc: 0.5558 - val_loss: 0.1542 - val_acc: 0.9635 - val_auc: 0.5562\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.15414\n",
      "Epoch 33/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1551 - acc: 0.9635 - auc: 0.5567 - val_loss: 0.1541 - val_acc: 0.9635 - val_auc: 0.5572\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.15414 to 0.15412, saving model to best.h5\n",
      "Epoch 34/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1553 - acc: 0.9635 - auc: 0.5576 - val_loss: 0.1541 - val_acc: 0.9635 - val_auc: 0.5580\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.15412\n",
      "Epoch 35/1000\n",
      "291698/291698 [==============================] - 34s 118us/step - loss: 0.1551 - acc: 0.9635 - auc: 0.5584 - val_loss: 0.1542 - val_acc: 0.9635 - val_auc: 0.5588\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.15412\n",
      "Epoch 36/1000\n",
      "291698/291698 [==============================] - 34s 117us/step - loss: 0.1550 - acc: 0.9635 - auc: 0.5592 - val_loss: 0.1539 - val_acc: 0.9635 - val_auc: 0.5597\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.15412 to 0.15392, saving model to best.h5\n",
      "Epoch 37/1000\n",
      "291698/291698 [==============================] - 34s 118us/step - loss: 0.1551 - acc: 0.9635 - auc: 0.5600 - val_loss: 0.1540 - val_acc: 0.9635 - val_auc: 0.5604\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.15392\n",
      "Epoch 38/1000\n",
      "291698/291698 [==============================] - 34s 117us/step - loss: 0.1552 - acc: 0.9635 - auc: 0.5608 - val_loss: 0.1539 - val_acc: 0.9635 - val_auc: 0.5612\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.15392 to 0.15386, saving model to best.h5\n",
      "Epoch 39/1000\n",
      "291698/291698 [==============================] - 34s 115us/step - loss: 0.1551 - acc: 0.9635 - auc: 0.5615 - val_loss: 0.1542 - val_acc: 0.9635 - val_auc: 0.5619\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.15386\n",
      "Epoch 40/1000\n",
      "291698/291698 [==============================] - 34s 115us/step - loss: 0.1551 - acc: 0.9635 - auc: 0.5622 - val_loss: 0.1540 - val_acc: 0.9635 - val_auc: 0.5625\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.15386\n",
      "Epoch 41/1000\n",
      "291698/291698 [==============================] - 34s 118us/step - loss: 0.1552 - acc: 0.9635 - auc: 0.5628 - val_loss: 0.1539 - val_acc: 0.9635 - val_auc: 0.5631\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.15386\n",
      "Epoch 42/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1550 - acc: 0.9635 - auc: 0.5635 - val_loss: 0.1539 - val_acc: 0.9635 - val_auc: 0.5638\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.15386\n",
      "Epoch 43/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1551 - acc: 0.9635 - auc: 0.5641 - val_loss: 0.1541 - val_acc: 0.9635 - val_auc: 0.5644\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.15386\n",
      "Epoch 44/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1552 - acc: 0.9635 - auc: 0.5646 - val_loss: 0.1538 - val_acc: 0.9635 - val_auc: 0.5649\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.15386 to 0.15384, saving model to best.h5\n",
      "Epoch 45/1000\n",
      "291698/291698 [==============================] - 34s 117us/step - loss: 0.1550 - acc: 0.9635 - auc: 0.5652 - val_loss: 0.1538 - val_acc: 0.9635 - val_auc: 0.5654\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.15384 to 0.15384, saving model to best.h5\n",
      "Epoch 46/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1549 - acc: 0.9635 - auc: 0.5657 - val_loss: 0.1538 - val_acc: 0.9635 - val_auc: 0.5660\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.15384\n",
      "Epoch 47/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1550 - acc: 0.9635 - auc: 0.5663 - val_loss: 0.1539 - val_acc: 0.9635 - val_auc: 0.5665\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.15384\n",
      "Epoch 48/1000\n",
      "291698/291698 [==============================] - 34s 116us/step - loss: 0.1550 - acc: 0.9635 - auc: 0.5668 - val_loss: 0.1540 - val_acc: 0.9635 - val_auc: 0.5671\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.15384\n",
      "Epoch 49/1000\n",
      "291698/291698 [==============================] - 34s 117us/step - loss: 0.1550 - acc: 0.9635 - auc: 0.5673 - val_loss: 0.1539 - val_acc: 0.9635 - val_auc: 0.5676\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.15384\n",
      "Epoch 50/1000\n",
      "291698/291698 [==============================] - 34s 116us/step - loss: 0.1549 - acc: 0.9635 - auc: 0.5678 - val_loss: 0.1539 - val_acc: 0.9635 - val_auc: 0.5681\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.15384\n",
      "Epoch 51/1000\n",
      "291698/291698 [==============================] - 34s 115us/step - loss: 0.1550 - acc: 0.9635 - auc: 0.5683 - val_loss: 0.1539 - val_acc: 0.9635 - val_auc: 0.5686\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.15384\n",
      "Epoch 52/1000\n",
      "291698/291698 [==============================] - 34s 116us/step - loss: 0.1549 - acc: 0.9635 - auc: 0.5688 - val_loss: 0.1538 - val_acc: 0.9635 - val_auc: 0.5691\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.15384\n",
      "Epoch 53/1000\n",
      "291698/291698 [==============================] - 35s 118us/step - loss: 0.1550 - acc: 0.9635 - auc: 0.5693 - val_loss: 0.1538 - val_acc: 0.9635 - val_auc: 0.5695\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.15384 to 0.15383, saving model to best.h5\n",
      "Epoch 54/1000\n",
      "291698/291698 [==============================] - 34s 117us/step - loss: 0.1550 - acc: 0.9635 - auc: 0.5697 - val_loss: 0.1539 - val_acc: 0.9635 - val_auc: 0.5699\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.15383\n",
      "Epoch 55/1000\n",
      "291698/291698 [==============================] - 34s 115us/step - loss: 0.1550 - acc: 0.9635 - auc: 0.5701 - val_loss: 0.1538 - val_acc: 0.9635 - val_auc: 0.5703\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.15383 to 0.15380, saving model to best.h5\n",
      "Epoch 56/1000\n",
      "291698/291698 [==============================] - 34s 115us/step - loss: 0.1549 - acc: 0.9635 - auc: 0.5706 - val_loss: 0.1538 - val_acc: 0.9635 - val_auc: 0.5708\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.15380\n",
      "Epoch 57/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1550 - acc: 0.9635 - auc: 0.5710 - val_loss: 0.1538 - val_acc: 0.9635 - val_auc: 0.5711\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.15380\n",
      "Epoch 58/1000\n",
      "291698/291698 [==============================] - 35s 118us/step - loss: 0.1550 - acc: 0.9635 - auc: 0.5713 - val_loss: 0.1538 - val_acc: 0.9635 - val_auc: 0.5715\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.15380\n",
      "Epoch 59/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1550 - acc: 0.9635 - auc: 0.5717 - val_loss: 0.1538 - val_acc: 0.9635 - val_auc: 0.5719\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.15380\n",
      "Epoch 60/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1550 - acc: 0.9635 - auc: 0.5721 - val_loss: 0.1539 - val_acc: 0.9635 - val_auc: 0.5722\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.15380\n",
      "Epoch 61/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1549 - acc: 0.9635 - auc: 0.5724 - val_loss: 0.1538 - val_acc: 0.9635 - val_auc: 0.5726\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.15380\n",
      "Epoch 62/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1548 - acc: 0.9635 - auc: 0.5728 - val_loss: 0.1538 - val_acc: 0.9635 - val_auc: 0.5730\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.15380 to 0.15377, saving model to best.h5\n",
      "Epoch 63/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1549 - acc: 0.9635 - auc: 0.5731 - val_loss: 0.1538 - val_acc: 0.9635 - val_auc: 0.5733\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.15377 to 0.15377, saving model to best.h5\n",
      "Epoch 64/1000\n",
      "291698/291698 [==============================] - 35s 120us/step - loss: 0.1550 - acc: 0.9635 - auc: 0.5735 - val_loss: 0.1540 - val_acc: 0.9635 - val_auc: 0.5736\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.15377\n",
      "Epoch 65/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1549 - acc: 0.9635 - auc: 0.5737 - val_loss: 0.1537 - val_acc: 0.9635 - val_auc: 0.5739\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.15377 to 0.15374, saving model to best.h5\n",
      "Epoch 66/1000\n",
      "291698/291698 [==============================] - 35s 120us/step - loss: 0.1548 - acc: 0.9635 - auc: 0.5740 - val_loss: 0.1539 - val_acc: 0.9635 - val_auc: 0.5742\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.15374\n",
      "Epoch 67/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1548 - acc: 0.9635 - auc: 0.5744 - val_loss: 0.1538 - val_acc: 0.9635 - val_auc: 0.5745\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.15374\n",
      "Epoch 68/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1548 - acc: 0.9635 - auc: 0.5747 - val_loss: 0.1538 - val_acc: 0.9635 - val_auc: 0.5748\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.15374\n",
      "Epoch 69/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1548 - acc: 0.9635 - auc: 0.5750 - val_loss: 0.1537 - val_acc: 0.9635 - val_auc: 0.5751\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.15374 to 0.15374, saving model to best.h5\n",
      "Epoch 70/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1548 - acc: 0.9635 - auc: 0.5753 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5754\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.15374 to 0.15364, saving model to best.h5\n",
      "Epoch 71/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1550 - acc: 0.9635 - auc: 0.5756 - val_loss: 0.1539 - val_acc: 0.9635 - val_auc: 0.5757\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.15364\n",
      "Epoch 72/1000\n",
      "291698/291698 [==============================] - 34s 118us/step - loss: 0.1550 - acc: 0.9635 - auc: 0.5758 - val_loss: 0.1539 - val_acc: 0.9635 - val_auc: 0.5759\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.15364\n",
      "Epoch 73/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1548 - acc: 0.9635 - auc: 0.5760 - val_loss: 0.1537 - val_acc: 0.9635 - val_auc: 0.5762\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.15364\n",
      "Epoch 74/1000\n",
      "291698/291698 [==============================] - 34s 118us/step - loss: 0.1549 - acc: 0.9635 - auc: 0.5763 - val_loss: 0.1537 - val_acc: 0.9635 - val_auc: 0.5764\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.15364\n",
      "Epoch 75/1000\n",
      "291698/291698 [==============================] - 34s 118us/step - loss: 0.1549 - acc: 0.9635 - auc: 0.5765 - val_loss: 0.1537 - val_acc: 0.9635 - val_auc: 0.5767\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.15364\n",
      "Epoch 76/1000\n",
      "291698/291698 [==============================] - 34s 118us/step - loss: 0.1550 - acc: 0.9635 - auc: 0.5768 - val_loss: 0.1538 - val_acc: 0.9635 - val_auc: 0.5769\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.15364\n",
      "Epoch 77/1000\n",
      "291698/291698 [==============================] - 34s 118us/step - loss: 0.1549 - acc: 0.9635 - auc: 0.5770 - val_loss: 0.1538 - val_acc: 0.9635 - val_auc: 0.5771\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.15364\n",
      "Epoch 78/1000\n",
      "291698/291698 [==============================] - 34s 118us/step - loss: 0.1548 - acc: 0.9635 - auc: 0.5772 - val_loss: 0.1539 - val_acc: 0.9635 - val_auc: 0.5773\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.15364\n",
      "Epoch 79/1000\n",
      "291698/291698 [==============================] - 34s 118us/step - loss: 0.1549 - acc: 0.9635 - auc: 0.5774 - val_loss: 0.1538 - val_acc: 0.9635 - val_auc: 0.5775\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.15364\n",
      "Epoch 80/1000\n",
      "291698/291698 [==============================] - 34s 118us/step - loss: 0.1548 - acc: 0.9635 - auc: 0.5776 - val_loss: 0.1537 - val_acc: 0.9635 - val_auc: 0.5778\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.15364\n",
      "Epoch 81/1000\n",
      "291698/291698 [==============================] - 34s 118us/step - loss: 0.1548 - acc: 0.9635 - auc: 0.5779 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5780\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.15364\n",
      "Epoch 82/1000\n",
      "291698/291698 [==============================] - 34s 118us/step - loss: 0.1549 - acc: 0.9635 - auc: 0.5781 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5782\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.15364 to 0.15363, saving model to best.h5\n",
      "Epoch 83/1000\n",
      "291698/291698 [==============================] - 34s 118us/step - loss: 0.1548 - acc: 0.9635 - auc: 0.5783 - val_loss: 0.1538 - val_acc: 0.9635 - val_auc: 0.5784\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.15363\n",
      "Epoch 84/1000\n",
      "291698/291698 [==============================] - 34s 117us/step - loss: 0.1548 - acc: 0.9635 - auc: 0.5785 - val_loss: 0.1539 - val_acc: 0.9635 - val_auc: 0.5786\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.15363\n",
      "Epoch 85/1000\n",
      "291698/291698 [==============================] - 35s 118us/step - loss: 0.1548 - acc: 0.9635 - auc: 0.5787 - val_loss: 0.1537 - val_acc: 0.9635 - val_auc: 0.5788\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.15363\n",
      "Epoch 86/1000\n",
      "291698/291698 [==============================] - 35s 118us/step - loss: 0.1547 - acc: 0.9635 - auc: 0.5789 - val_loss: 0.1539 - val_acc: 0.9635 - val_auc: 0.5790\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.15363\n",
      "Epoch 87/1000\n",
      "291698/291698 [==============================] - 35s 118us/step - loss: 0.1547 - acc: 0.9635 - auc: 0.5791 - val_loss: 0.1537 - val_acc: 0.9635 - val_auc: 0.5792\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.15363\n",
      "Epoch 88/1000\n",
      "291698/291698 [==============================] - 35s 118us/step - loss: 0.1549 - acc: 0.9635 - auc: 0.5793 - val_loss: 0.1538 - val_acc: 0.9635 - val_auc: 0.5793\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.15363\n",
      "Epoch 89/1000\n",
      "291698/291698 [==============================] - 34s 118us/step - loss: 0.1548 - acc: 0.9635 - auc: 0.5794 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5795\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.15363 to 0.15358, saving model to best.h5\n",
      "Epoch 90/1000\n",
      "291698/291698 [==============================] - 34s 118us/step - loss: 0.1548 - acc: 0.9635 - auc: 0.5796 - val_loss: 0.1538 - val_acc: 0.9635 - val_auc: 0.5797\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.15358\n",
      "Epoch 91/1000\n",
      "291698/291698 [==============================] - 34s 118us/step - loss: 0.1548 - acc: 0.9635 - auc: 0.5798 - val_loss: 0.1538 - val_acc: 0.9635 - val_auc: 0.5799\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.15358\n",
      "Epoch 92/1000\n",
      "291698/291698 [==============================] - 34s 118us/step - loss: 0.1550 - acc: 0.9635 - auc: 0.5799 - val_loss: 0.1538 - val_acc: 0.9635 - val_auc: 0.5800\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.15358\n",
      "Epoch 93/1000\n",
      "291698/291698 [==============================] - 34s 118us/step - loss: 0.1548 - acc: 0.9635 - auc: 0.5801 - val_loss: 0.1537 - val_acc: 0.9635 - val_auc: 0.5802\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.15358\n",
      "Epoch 94/1000\n",
      "291698/291698 [==============================] - 34s 118us/step - loss: 0.1548 - acc: 0.9635 - auc: 0.5803 - val_loss: 0.1537 - val_acc: 0.9635 - val_auc: 0.5803\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.15358\n",
      "Epoch 95/1000\n",
      "291698/291698 [==============================] - 35s 118us/step - loss: 0.1548 - acc: 0.9635 - auc: 0.5804 - val_loss: 0.1538 - val_acc: 0.9635 - val_auc: 0.5805\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.15358\n",
      "Epoch 96/1000\n",
      "291698/291698 [==============================] - 34s 118us/step - loss: 0.1547 - acc: 0.9635 - auc: 0.5806 - val_loss: 0.1537 - val_acc: 0.9635 - val_auc: 0.5807\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.15358\n",
      "Epoch 97/1000\n",
      "291698/291698 [==============================] - 35s 118us/step - loss: 0.1548 - acc: 0.9635 - auc: 0.5808 - val_loss: 0.1537 - val_acc: 0.9635 - val_auc: 0.5808\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.15358\n",
      "Epoch 98/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1549 - acc: 0.9635 - auc: 0.5809 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5810\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.15358\n",
      "Epoch 99/1000\n",
      "291698/291698 [==============================] - 34s 118us/step - loss: 0.1547 - acc: 0.9635 - auc: 0.5811 - val_loss: 0.1537 - val_acc: 0.9635 - val_auc: 0.5811\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.15358\n",
      "Epoch 100/1000\n",
      "291698/291698 [==============================] - 34s 117us/step - loss: 0.1547 - acc: 0.9635 - auc: 0.5812 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5813\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.15358\n",
      "Epoch 101/1000\n",
      "291698/291698 [==============================] - 35s 118us/step - loss: 0.1548 - acc: 0.9635 - auc: 0.5814 - val_loss: 0.1537 - val_acc: 0.9635 - val_auc: 0.5815\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.15358\n",
      "Epoch 102/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1546 - acc: 0.9635 - auc: 0.5815 - val_loss: 0.1537 - val_acc: 0.9635 - val_auc: 0.5816\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.15358\n",
      "Epoch 103/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1548 - acc: 0.9635 - auc: 0.5817 - val_loss: 0.1538 - val_acc: 0.9635 - val_auc: 0.5818\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.15358\n",
      "Epoch 104/1000\n",
      "291698/291698 [==============================] - 35s 118us/step - loss: 0.1548 - acc: 0.9635 - auc: 0.5819 - val_loss: 0.1539 - val_acc: 0.9635 - val_auc: 0.5819\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.15358\n",
      "Epoch 105/1000\n",
      "291698/291698 [==============================] - 34s 118us/step - loss: 0.1548 - acc: 0.9635 - auc: 0.5820 - val_loss: 0.1538 - val_acc: 0.9635 - val_auc: 0.5820\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.15358\n",
      "Epoch 106/1000\n",
      "291698/291698 [==============================] - 35s 118us/step - loss: 0.1548 - acc: 0.9635 - auc: 0.5821 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5822\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.15358\n",
      "Epoch 107/1000\n",
      "291698/291698 [==============================] - 34s 118us/step - loss: 0.1547 - acc: 0.9635 - auc: 0.5822 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5823\n",
      "\n",
      "Epoch 00107: val_loss improved from 0.15358 to 0.15356, saving model to best.h5\n",
      "Epoch 108/1000\n",
      "291698/291698 [==============================] - 35s 118us/step - loss: 0.1547 - acc: 0.9635 - auc: 0.5824 - val_loss: 0.1538 - val_acc: 0.9635 - val_auc: 0.5825\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.15356\n",
      "Epoch 109/1000\n",
      "291698/291698 [==============================] - 35s 118us/step - loss: 0.1548 - acc: 0.9635 - auc: 0.5825 - val_loss: 0.1537 - val_acc: 0.9635 - val_auc: 0.5826\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.15356\n",
      "Epoch 110/1000\n",
      "291698/291698 [==============================] - 34s 118us/step - loss: 0.1549 - acc: 0.9635 - auc: 0.5826 - val_loss: 0.1537 - val_acc: 0.9635 - val_auc: 0.5827\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.15356\n",
      "Epoch 111/1000\n",
      "291698/291698 [==============================] - 34s 118us/step - loss: 0.1547 - acc: 0.9635 - auc: 0.5827 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5828\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.15356\n",
      "Epoch 112/1000\n",
      "291698/291698 [==============================] - 35s 118us/step - loss: 0.1547 - acc: 0.9635 - auc: 0.5829 - val_loss: 0.1538 - val_acc: 0.9635 - val_auc: 0.5830\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.15356\n",
      "Epoch 113/1000\n",
      "291698/291698 [==============================] - 34s 118us/step - loss: 0.1547 - acc: 0.9635 - auc: 0.5830 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5831\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.15356\n",
      "Epoch 114/1000\n",
      "291698/291698 [==============================] - 34s 118us/step - loss: 0.1547 - acc: 0.9635 - auc: 0.5831 - val_loss: 0.1537 - val_acc: 0.9635 - val_auc: 0.5832\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.15356\n",
      "Epoch 115/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1547 - acc: 0.9635 - auc: 0.5833 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5833\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.15356\n",
      "Epoch 116/1000\n",
      "291698/291698 [==============================] - 34s 118us/step - loss: 0.1548 - acc: 0.9635 - auc: 0.5834 - val_loss: 0.1537 - val_acc: 0.9635 - val_auc: 0.5834\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.15356\n",
      "Epoch 117/1000\n",
      "291698/291698 [==============================] - 34s 118us/step - loss: 0.1548 - acc: 0.9635 - auc: 0.5835 - val_loss: 0.1539 - val_acc: 0.9635 - val_auc: 0.5835\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.15356\n",
      "Epoch 118/1000\n",
      "291698/291698 [==============================] - 34s 118us/step - loss: 0.1548 - acc: 0.9635 - auc: 0.5836 - val_loss: 0.1538 - val_acc: 0.9635 - val_auc: 0.5836\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.15356\n",
      "Epoch 119/1000\n",
      "291698/291698 [==============================] - 34s 118us/step - loss: 0.1549 - acc: 0.9635 - auc: 0.5837 - val_loss: 0.1537 - val_acc: 0.9635 - val_auc: 0.5837\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.15356\n",
      "Epoch 120/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1548 - acc: 0.9635 - auc: 0.5838 - val_loss: 0.1538 - val_acc: 0.9635 - val_auc: 0.5838\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.15356\n",
      "Epoch 121/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1549 - acc: 0.9635 - auc: 0.5839 - val_loss: 0.1537 - val_acc: 0.9635 - val_auc: 0.5839\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.15356\n",
      "Epoch 122/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1546 - acc: 0.9635 - auc: 0.5840 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5841\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.15356\n",
      "Epoch 123/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1547 - acc: 0.9635 - auc: 0.5841 - val_loss: 0.1537 - val_acc: 0.9635 - val_auc: 0.5842\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.15356\n",
      "Epoch 124/1000\n",
      "291698/291698 [==============================] - 35s 120us/step - loss: 0.1548 - acc: 0.9635 - auc: 0.5842 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5843\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.15356\n",
      "Epoch 125/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1546 - acc: 0.9635 - auc: 0.5843 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5844\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.15356\n",
      "Epoch 126/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1548 - acc: 0.9635 - auc: 0.5845 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5845\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.15356\n",
      "Epoch 127/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1549 - acc: 0.9635 - auc: 0.5846 - val_loss: 0.1538 - val_acc: 0.9635 - val_auc: 0.5846\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.15356\n",
      "Epoch 128/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1546 - acc: 0.9635 - auc: 0.5847 - val_loss: 0.1537 - val_acc: 0.9635 - val_auc: 0.5847\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.15356\n",
      "Epoch 129/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1547 - acc: 0.9635 - auc: 0.5847 - val_loss: 0.1537 - val_acc: 0.9635 - val_auc: 0.5848\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.15356\n",
      "Epoch 130/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1546 - acc: 0.9635 - auc: 0.5848 - val_loss: 0.1539 - val_acc: 0.9635 - val_auc: 0.5849\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.15356\n",
      "Epoch 131/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1546 - acc: 0.9635 - auc: 0.5849 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5850\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.15356\n",
      "Epoch 132/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1547 - acc: 0.9635 - auc: 0.5850 - val_loss: 0.1537 - val_acc: 0.9635 - val_auc: 0.5851\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.15356\n",
      "Epoch 133/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1548 - acc: 0.9635 - auc: 0.5851 - val_loss: 0.1538 - val_acc: 0.9635 - val_auc: 0.5852\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.15356\n",
      "Epoch 134/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1546 - acc: 0.9635 - auc: 0.5852 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5852\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.15356\n",
      "Epoch 135/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1547 - acc: 0.9635 - auc: 0.5853 - val_loss: 0.1538 - val_acc: 0.9635 - val_auc: 0.5853\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.15356\n",
      "Epoch 136/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1546 - acc: 0.9635 - auc: 0.5854 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5854\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.15356\n",
      "Epoch 137/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1548 - acc: 0.9635 - auc: 0.5855 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5855\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.15356\n",
      "Epoch 138/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1546 - acc: 0.9635 - auc: 0.5856 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5856\n",
      "\n",
      "Epoch 00138: val_loss improved from 0.15356 to 0.15355, saving model to best.h5\n",
      "Epoch 139/1000\n",
      "291698/291698 [==============================] - 35s 120us/step - loss: 0.1547 - acc: 0.9635 - auc: 0.5857 - val_loss: 0.1537 - val_acc: 0.9635 - val_auc: 0.5857\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.15355\n",
      "Epoch 140/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1548 - acc: 0.9635 - auc: 0.5857 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5858\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.15355\n",
      "Epoch 141/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1546 - acc: 0.9635 - auc: 0.5858 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5859\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.15355\n",
      "Epoch 142/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1546 - acc: 0.9635 - auc: 0.5859 - val_loss: 0.1537 - val_acc: 0.9635 - val_auc: 0.5860\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.15355\n",
      "Epoch 143/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1546 - acc: 0.9635 - auc: 0.5860 - val_loss: 0.1537 - val_acc: 0.9635 - val_auc: 0.5860\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.15355\n",
      "Epoch 144/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1547 - acc: 0.9635 - auc: 0.5861 - val_loss: 0.1538 - val_acc: 0.9635 - val_auc: 0.5861\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.15355\n",
      "Epoch 145/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1547 - acc: 0.9635 - auc: 0.5862 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5862\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.15355\n",
      "Epoch 146/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1546 - acc: 0.9635 - auc: 0.5863 - val_loss: 0.1537 - val_acc: 0.9635 - val_auc: 0.5863\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.15355\n",
      "Epoch 147/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1549 - acc: 0.9635 - auc: 0.5863 - val_loss: 0.1538 - val_acc: 0.9635 - val_auc: 0.5863\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.15355\n",
      "Epoch 148/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1547 - acc: 0.9635 - auc: 0.5864 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5864\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.15355\n",
      "Epoch 149/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1546 - acc: 0.9635 - auc: 0.5865 - val_loss: 0.1537 - val_acc: 0.9635 - val_auc: 0.5865\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.15355\n",
      "Epoch 150/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1547 - acc: 0.9635 - auc: 0.5865 - val_loss: 0.1538 - val_acc: 0.9635 - val_auc: 0.5866\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.15355\n",
      "Epoch 151/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1547 - acc: 0.9635 - auc: 0.5866 - val_loss: 0.1537 - val_acc: 0.9635 - val_auc: 0.5866\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.15355\n",
      "Epoch 152/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1547 - acc: 0.9635 - auc: 0.5867 - val_loss: 0.1538 - val_acc: 0.9635 - val_auc: 0.5867\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.15355\n",
      "Epoch 153/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1547 - acc: 0.9635 - auc: 0.5867 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5868\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.15355\n",
      "Epoch 154/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1546 - acc: 0.9635 - auc: 0.5868 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5869\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.15355\n",
      "Epoch 155/1000\n",
      "291698/291698 [==============================] - 35s 120us/step - loss: 0.1546 - acc: 0.9635 - auc: 0.5869 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5869\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.15355\n",
      "Epoch 156/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1546 - acc: 0.9635 - auc: 0.5870 - val_loss: 0.1537 - val_acc: 0.9635 - val_auc: 0.5870\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.15355\n",
      "Epoch 157/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1546 - acc: 0.9635 - auc: 0.5870 - val_loss: 0.1535 - val_acc: 0.9635 - val_auc: 0.5871\n",
      "\n",
      "Epoch 00157: val_loss improved from 0.15355 to 0.15355, saving model to best.h5\n",
      "Epoch 158/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1546 - acc: 0.9635 - auc: 0.5871 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5872\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.15355\n",
      "Epoch 159/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1548 - acc: 0.9635 - auc: 0.5872 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5872\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.15355\n",
      "Epoch 160/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1547 - acc: 0.9635 - auc: 0.5873 - val_loss: 0.1537 - val_acc: 0.9635 - val_auc: 0.5873\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.15355\n",
      "Epoch 161/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1547 - acc: 0.9635 - auc: 0.5873 - val_loss: 0.1538 - val_acc: 0.9635 - val_auc: 0.5874\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.15355\n",
      "Epoch 162/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1545 - acc: 0.9635 - auc: 0.5874 - val_loss: 0.1535 - val_acc: 0.9635 - val_auc: 0.5874\n",
      "\n",
      "Epoch 00162: val_loss improved from 0.15355 to 0.15354, saving model to best.h5\n",
      "Epoch 163/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1547 - acc: 0.9635 - auc: 0.5875 - val_loss: 0.1537 - val_acc: 0.9635 - val_auc: 0.5875\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.15354\n",
      "Epoch 164/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1547 - acc: 0.9635 - auc: 0.5875 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5876\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.15354\n",
      "Epoch 165/1000\n",
      "291698/291698 [==============================] - 35s 120us/step - loss: 0.1547 - acc: 0.9635 - auc: 0.5876 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5877\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.15354\n",
      "Epoch 166/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1547 - acc: 0.9635 - auc: 0.5877 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5877\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.15354\n",
      "Epoch 167/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1548 - acc: 0.9635 - auc: 0.5878 - val_loss: 0.1537 - val_acc: 0.9635 - val_auc: 0.5878\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.15354\n",
      "Epoch 168/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1547 - acc: 0.9635 - auc: 0.5878 - val_loss: 0.1537 - val_acc: 0.9635 - val_auc: 0.5878\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.15354\n",
      "Epoch 169/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1545 - acc: 0.9635 - auc: 0.5879 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5879\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.15354\n",
      "Epoch 170/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1546 - acc: 0.9635 - auc: 0.5879 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5880\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.15354\n",
      "Epoch 171/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1546 - acc: 0.9635 - auc: 0.5880 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5880\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.15354\n",
      "Epoch 172/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1547 - acc: 0.9635 - auc: 0.5881 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5881\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.15354\n",
      "Epoch 173/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1548 - acc: 0.9635 - auc: 0.5881 - val_loss: 0.1537 - val_acc: 0.9635 - val_auc: 0.5882\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.15354\n",
      "Epoch 174/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1547 - acc: 0.9635 - auc: 0.5882 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5882\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.15354\n",
      "Epoch 175/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1547 - acc: 0.9635 - auc: 0.5882 - val_loss: 0.1535 - val_acc: 0.9635 - val_auc: 0.5883\n",
      "\n",
      "Epoch 00175: val_loss improved from 0.15354 to 0.15354, saving model to best.h5\n",
      "Epoch 176/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1546 - acc: 0.9635 - auc: 0.5883 - val_loss: 0.1535 - val_acc: 0.9635 - val_auc: 0.5884\n",
      "\n",
      "Epoch 00176: val_loss improved from 0.15354 to 0.15353, saving model to best.h5\n",
      "Epoch 177/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1546 - acc: 0.9635 - auc: 0.5884 - val_loss: 0.1535 - val_acc: 0.9635 - val_auc: 0.5884\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.15353\n",
      "Epoch 178/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1546 - acc: 0.9635 - auc: 0.5885 - val_loss: 0.1537 - val_acc: 0.9635 - val_auc: 0.5885\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.15353\n",
      "Epoch 179/1000\n",
      "291698/291698 [==============================] - 35s 120us/step - loss: 0.1547 - acc: 0.9635 - auc: 0.5885 - val_loss: 0.1538 - val_acc: 0.9635 - val_auc: 0.5885\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.15353\n",
      "Epoch 180/1000\n",
      "291698/291698 [==============================] - 35s 120us/step - loss: 0.1546 - acc: 0.9635 - auc: 0.5885 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5886\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.15353\n",
      "Epoch 181/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1547 - acc: 0.9635 - auc: 0.5886 - val_loss: 0.1537 - val_acc: 0.9635 - val_auc: 0.5886\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.15353\n",
      "Epoch 182/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1547 - acc: 0.9635 - auc: 0.5887 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5887\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.15353\n",
      "Epoch 183/1000\n",
      "291698/291698 [==============================] - 35s 120us/step - loss: 0.1546 - acc: 0.9635 - auc: 0.5887 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5888\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.15353\n",
      "Epoch 184/1000\n",
      "291698/291698 [==============================] - 35s 120us/step - loss: 0.1546 - acc: 0.9635 - auc: 0.5888 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5888\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.15353\n",
      "Epoch 185/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1546 - acc: 0.9635 - auc: 0.5889 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5889\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.15353\n",
      "Epoch 186/1000\n",
      "291698/291698 [==============================] - 35s 120us/step - loss: 0.1546 - acc: 0.9635 - auc: 0.5889 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5889\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.15353\n",
      "Epoch 187/1000\n",
      "291698/291698 [==============================] - 35s 120us/step - loss: 0.1547 - acc: 0.9635 - auc: 0.5890 - val_loss: 0.1535 - val_acc: 0.9635 - val_auc: 0.5890\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.15353\n",
      "Epoch 188/1000\n",
      "291698/291698 [==============================] - 35s 120us/step - loss: 0.1546 - acc: 0.9635 - auc: 0.5890 - val_loss: 0.1537 - val_acc: 0.9635 - val_auc: 0.5890\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.15353\n",
      "Epoch 189/1000\n",
      "291698/291698 [==============================] - 35s 120us/step - loss: 0.1545 - acc: 0.9635 - auc: 0.5891 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5891\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.15353\n",
      "Epoch 190/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1547 - acc: 0.9635 - auc: 0.5891 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5891\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.15353\n",
      "Epoch 191/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1548 - acc: 0.9635 - auc: 0.5892 - val_loss: 0.1537 - val_acc: 0.9635 - val_auc: 0.5892\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.15353\n",
      "Epoch 192/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1547 - acc: 0.9635 - auc: 0.5892 - val_loss: 0.1537 - val_acc: 0.9635 - val_auc: 0.5892\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.15353\n",
      "Epoch 193/1000\n",
      "291698/291698 [==============================] - 35s 120us/step - loss: 0.1546 - acc: 0.9635 - auc: 0.5892 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5893\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.15353\n",
      "Epoch 194/1000\n",
      "291698/291698 [==============================] - 35s 120us/step - loss: 0.1545 - acc: 0.9635 - auc: 0.5893 - val_loss: 0.1535 - val_acc: 0.9635 - val_auc: 0.5893\n",
      "\n",
      "Epoch 00194: val_loss improved from 0.15353 to 0.15352, saving model to best.h5\n",
      "Epoch 195/1000\n",
      "291698/291698 [==============================] - 35s 120us/step - loss: 0.1546 - acc: 0.9635 - auc: 0.5894 - val_loss: 0.1537 - val_acc: 0.9635 - val_auc: 0.5894\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.15352\n",
      "Epoch 196/1000\n",
      "291698/291698 [==============================] - 35s 120us/step - loss: 0.1545 - acc: 0.9635 - auc: 0.5894 - val_loss: 0.1535 - val_acc: 0.9635 - val_auc: 0.5894\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.15352\n",
      "Epoch 197/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1547 - acc: 0.9635 - auc: 0.5895 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5895\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.15352\n",
      "Epoch 198/1000\n",
      "291698/291698 [==============================] - 34s 117us/step - loss: 0.1545 - acc: 0.9635 - auc: 0.5895 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5896\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.15352\n",
      "Epoch 199/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1545 - acc: 0.9635 - auc: 0.5896 - val_loss: 0.1535 - val_acc: 0.9635 - val_auc: 0.5896\n",
      "\n",
      "Epoch 00199: val_loss improved from 0.15352 to 0.15347, saving model to best.h5\n",
      "Epoch 200/1000\n",
      "291698/291698 [==============================] - 35s 118us/step - loss: 0.1546 - acc: 0.9635 - auc: 0.5896 - val_loss: 0.1535 - val_acc: 0.9635 - val_auc: 0.5897\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.15347\n",
      "Epoch 201/1000\n",
      "291698/291698 [==============================] - 35s 118us/step - loss: 0.1547 - acc: 0.9635 - auc: 0.5897 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5897\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 0.15347\n",
      "Epoch 202/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1545 - acc: 0.9635 - auc: 0.5898 - val_loss: 0.1537 - val_acc: 0.9635 - val_auc: 0.5898\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 0.15347\n",
      "Epoch 203/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1547 - acc: 0.9635 - auc: 0.5898 - val_loss: 0.1538 - val_acc: 0.9635 - val_auc: 0.5898\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 0.15347\n",
      "Epoch 204/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1546 - acc: 0.9635 - auc: 0.5898 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5899\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 0.15347\n",
      "Epoch 205/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1544 - acc: 0.9635 - auc: 0.5899 - val_loss: 0.1535 - val_acc: 0.9635 - val_auc: 0.5899\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 0.15347\n",
      "Epoch 206/1000\n",
      "291698/291698 [==============================] - 35s 118us/step - loss: 0.1546 - acc: 0.9635 - auc: 0.5900 - val_loss: 0.1535 - val_acc: 0.9635 - val_auc: 0.5900\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 0.15347\n",
      "Epoch 207/1000\n",
      "291698/291698 [==============================] - 34s 117us/step - loss: 0.1547 - acc: 0.9635 - auc: 0.5900 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5900\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 0.15347\n",
      "Epoch 208/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1547 - acc: 0.9635 - auc: 0.5901 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5901\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 0.15347\n",
      "Epoch 209/1000\n",
      "291698/291698 [==============================] - 35s 120us/step - loss: 0.1547 - acc: 0.9635 - auc: 0.5901 - val_loss: 0.1538 - val_acc: 0.9635 - val_auc: 0.5901\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 0.15347\n",
      "Epoch 210/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1546 - acc: 0.9635 - auc: 0.5901 - val_loss: 0.1537 - val_acc: 0.9635 - val_auc: 0.5901\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 0.15347\n",
      "Epoch 211/1000\n",
      "291698/291698 [==============================] - 35s 120us/step - loss: 0.1546 - acc: 0.9635 - auc: 0.5902 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5902\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 0.15347\n",
      "Epoch 212/1000\n",
      "291698/291698 [==============================] - 35s 120us/step - loss: 0.1545 - acc: 0.9635 - auc: 0.5902 - val_loss: 0.1534 - val_acc: 0.9635 - val_auc: 0.5902\n",
      "\n",
      "Epoch 00212: val_loss improved from 0.15347 to 0.15344, saving model to best.h5\n",
      "Epoch 213/1000\n",
      "291698/291698 [==============================] - 35s 120us/step - loss: 0.1547 - acc: 0.9635 - auc: 0.5903 - val_loss: 0.1537 - val_acc: 0.9635 - val_auc: 0.5903\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 0.15344\n",
      "Epoch 214/1000\n",
      "291698/291698 [==============================] - 35s 120us/step - loss: 0.1545 - acc: 0.9635 - auc: 0.5903 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5904\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 0.15344\n",
      "Epoch 215/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1546 - acc: 0.9635 - auc: 0.5904 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5904\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 0.15344\n",
      "Epoch 216/1000\n",
      "291698/291698 [==============================] - 35s 120us/step - loss: 0.1546 - acc: 0.9635 - auc: 0.5904 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5905\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 0.15344\n",
      "Epoch 217/1000\n",
      "291698/291698 [==============================] - 35s 120us/step - loss: 0.1547 - acc: 0.9635 - auc: 0.5905 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5905\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 0.15344\n",
      "Epoch 218/1000\n",
      "291698/291698 [==============================] - 35s 120us/step - loss: 0.1547 - acc: 0.9635 - auc: 0.5905 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5905\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 0.15344\n",
      "Epoch 219/1000\n",
      "291698/291698 [==============================] - 35s 120us/step - loss: 0.1546 - acc: 0.9635 - auc: 0.5906 - val_loss: 0.1535 - val_acc: 0.9635 - val_auc: 0.5906\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 0.15344\n",
      "Epoch 220/1000\n",
      "291698/291698 [==============================] - 35s 120us/step - loss: 0.1545 - acc: 0.9635 - auc: 0.5906 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5906\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 0.15344\n",
      "Epoch 221/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1547 - acc: 0.9635 - auc: 0.5907 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5907\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 0.15344\n",
      "Epoch 222/1000\n",
      "291698/291698 [==============================] - 35s 120us/step - loss: 0.1546 - acc: 0.9635 - auc: 0.5907 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5907\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 0.15344\n",
      "Epoch 223/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1544 - acc: 0.9635 - auc: 0.5908 - val_loss: 0.1535 - val_acc: 0.9635 - val_auc: 0.5908\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 0.15344\n",
      "Epoch 224/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1545 - acc: 0.9635 - auc: 0.5908 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5908\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 0.15344\n",
      "Epoch 225/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1546 - acc: 0.9635 - auc: 0.5909 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5909\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 0.15344\n",
      "Epoch 226/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1547 - acc: 0.9635 - auc: 0.5909 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5909\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 0.15344\n",
      "Epoch 227/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1545 - acc: 0.9635 - auc: 0.5909 - val_loss: 0.1537 - val_acc: 0.9635 - val_auc: 0.5910\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 0.15344\n",
      "Epoch 228/1000\n",
      "291698/291698 [==============================] - 34s 118us/step - loss: 0.1546 - acc: 0.9635 - auc: 0.5910 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5910\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 0.15344\n",
      "Epoch 229/1000\n",
      "291698/291698 [==============================] - 34s 118us/step - loss: 0.1545 - acc: 0.9635 - auc: 0.5910 - val_loss: 0.1535 - val_acc: 0.9635 - val_auc: 0.5910\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 0.15344\n",
      "Epoch 230/1000\n",
      "291698/291698 [==============================] - 35s 118us/step - loss: 0.1546 - acc: 0.9635 - auc: 0.5911 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5911\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 0.15344\n",
      "Epoch 231/1000\n",
      "291698/291698 [==============================] - 34s 118us/step - loss: 0.1547 - acc: 0.9635 - auc: 0.5911 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5911\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 0.15344\n",
      "Epoch 232/1000\n",
      "291698/291698 [==============================] - 35s 118us/step - loss: 0.1545 - acc: 0.9635 - auc: 0.5911 - val_loss: 0.1535 - val_acc: 0.9635 - val_auc: 0.5912\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 0.15344\n",
      "Epoch 233/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1545 - acc: 0.9635 - auc: 0.5912 - val_loss: 0.1535 - val_acc: 0.9635 - val_auc: 0.5912\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 0.15344\n",
      "Epoch 234/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1545 - acc: 0.9635 - auc: 0.5912 - val_loss: 0.1535 - val_acc: 0.9635 - val_auc: 0.5913\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 0.15344\n",
      "Epoch 235/1000\n",
      "291698/291698 [==============================] - 35s 120us/step - loss: 0.1545 - acc: 0.9635 - auc: 0.5913 - val_loss: 0.1535 - val_acc: 0.9635 - val_auc: 0.5913\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 0.15344\n",
      "Epoch 236/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1545 - acc: 0.9635 - auc: 0.5913 - val_loss: 0.1534 - val_acc: 0.9635 - val_auc: 0.5914\n",
      "\n",
      "Epoch 00236: val_loss improved from 0.15344 to 0.15344, saving model to best.h5\n",
      "Epoch 237/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1544 - acc: 0.9635 - auc: 0.5914 - val_loss: 0.1535 - val_acc: 0.9635 - val_auc: 0.5914\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 0.15344\n",
      "Epoch 238/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1545 - acc: 0.9635 - auc: 0.5914 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5915\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 0.15344\n",
      "Epoch 239/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1544 - acc: 0.9635 - auc: 0.5915 - val_loss: 0.1535 - val_acc: 0.9635 - val_auc: 0.5915\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 0.15344\n",
      "Epoch 240/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1544 - acc: 0.9635 - auc: 0.5915 - val_loss: 0.1534 - val_acc: 0.9635 - val_auc: 0.5916\n",
      "\n",
      "Epoch 00240: val_loss improved from 0.15344 to 0.15341, saving model to best.h5\n",
      "Epoch 241/1000\n",
      "291698/291698 [==============================] - 35s 118us/step - loss: 0.1546 - acc: 0.9635 - auc: 0.5916 - val_loss: 0.1535 - val_acc: 0.9635 - val_auc: 0.5916\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 0.15341\n",
      "Epoch 242/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1545 - acc: 0.9635 - auc: 0.5916 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5916\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 0.15341\n",
      "Epoch 243/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1546 - acc: 0.9635 - auc: 0.5917 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5917\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 0.15341\n",
      "Epoch 244/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1546 - acc: 0.9635 - auc: 0.5917 - val_loss: 0.1535 - val_acc: 0.9635 - val_auc: 0.5917\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 0.15341\n",
      "Epoch 245/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1547 - acc: 0.9635 - auc: 0.5918 - val_loss: 0.1537 - val_acc: 0.9635 - val_auc: 0.5918\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 0.15341\n",
      "Epoch 246/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1543 - acc: 0.9635 - auc: 0.5918 - val_loss: 0.1534 - val_acc: 0.9635 - val_auc: 0.5918\n",
      "\n",
      "Epoch 00246: val_loss improved from 0.15341 to 0.15341, saving model to best.h5\n",
      "Epoch 247/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1546 - acc: 0.9635 - auc: 0.5918 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5918\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 0.15341\n",
      "Epoch 248/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1545 - acc: 0.9635 - auc: 0.5919 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5919\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 0.15341\n",
      "Epoch 249/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1545 - acc: 0.9635 - auc: 0.5919 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5919\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 0.15341\n",
      "Epoch 250/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1545 - acc: 0.9635 - auc: 0.5919 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5920\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 0.15341\n",
      "Epoch 251/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1544 - acc: 0.9635 - auc: 0.5920 - val_loss: 0.1535 - val_acc: 0.9635 - val_auc: 0.5920\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 0.15341\n",
      "Epoch 252/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1545 - acc: 0.9635 - auc: 0.5920 - val_loss: 0.1535 - val_acc: 0.9635 - val_auc: 0.5920\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 0.15341\n",
      "Epoch 253/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1546 - acc: 0.9635 - auc: 0.5921 - val_loss: 0.1535 - val_acc: 0.9635 - val_auc: 0.5921\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 0.15341\n",
      "Epoch 254/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1547 - acc: 0.9635 - auc: 0.5921 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5921\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 0.15341\n",
      "Epoch 255/1000\n",
      "291698/291698 [==============================] - 35s 118us/step - loss: 0.1546 - acc: 0.9635 - auc: 0.5922 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5922\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 0.15341\n",
      "Epoch 256/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1544 - acc: 0.9635 - auc: 0.5922 - val_loss: 0.1535 - val_acc: 0.9635 - val_auc: 0.5922\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 0.15341\n",
      "Epoch 257/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1546 - acc: 0.9635 - auc: 0.5922 - val_loss: 0.1535 - val_acc: 0.9635 - val_auc: 0.5922\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 0.15341\n",
      "Epoch 258/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1545 - acc: 0.9635 - auc: 0.5923 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5923\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 0.15341\n",
      "Epoch 259/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1545 - acc: 0.9635 - auc: 0.5923 - val_loss: 0.1535 - val_acc: 0.9635 - val_auc: 0.5923\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 0.15341\n",
      "Epoch 260/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1545 - acc: 0.9635 - auc: 0.5923 - val_loss: 0.1535 - val_acc: 0.9635 - val_auc: 0.5924\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 0.15341\n",
      "Epoch 261/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1546 - acc: 0.9635 - auc: 0.5924 - val_loss: 0.1535 - val_acc: 0.9635 - val_auc: 0.5924\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 0.15341\n",
      "Epoch 262/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1545 - acc: 0.9635 - auc: 0.5924 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5924\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 0.15341\n",
      "Epoch 263/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1546 - acc: 0.9635 - auc: 0.5924 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5925\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 0.15341\n",
      "Epoch 264/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1545 - acc: 0.9635 - auc: 0.5925 - val_loss: 0.1535 - val_acc: 0.9635 - val_auc: 0.5925\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 0.15341\n",
      "Epoch 265/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1545 - acc: 0.9635 - auc: 0.5925 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5925\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 0.15341\n",
      "Epoch 266/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1545 - acc: 0.9635 - auc: 0.5925 - val_loss: 0.1535 - val_acc: 0.9635 - val_auc: 0.5926\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 0.15341\n",
      "Epoch 267/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1544 - acc: 0.9635 - auc: 0.5926 - val_loss: 0.1535 - val_acc: 0.9635 - val_auc: 0.5926\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 0.15341\n",
      "Epoch 268/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1545 - acc: 0.9635 - auc: 0.5926 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5926\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 0.15341\n",
      "Epoch 269/1000\n",
      "291698/291698 [==============================] - 34s 118us/step - loss: 0.1547 - acc: 0.9635 - auc: 0.5927 - val_loss: 0.1535 - val_acc: 0.9635 - val_auc: 0.5927\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 0.15341\n",
      "Epoch 270/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1546 - acc: 0.9635 - auc: 0.5927 - val_loss: 0.1535 - val_acc: 0.9635 - val_auc: 0.5927\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 0.15341\n",
      "Epoch 271/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1544 - acc: 0.9635 - auc: 0.5927 - val_loss: 0.1535 - val_acc: 0.9635 - val_auc: 0.5927\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 0.15341\n",
      "Epoch 272/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1545 - acc: 0.9635 - auc: 0.5928 - val_loss: 0.1535 - val_acc: 0.9635 - val_auc: 0.5928\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 0.15341\n",
      "Epoch 273/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1545 - acc: 0.9635 - auc: 0.5928 - val_loss: 0.1535 - val_acc: 0.9635 - val_auc: 0.5928\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 0.15341\n",
      "Epoch 274/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1546 - acc: 0.9635 - auc: 0.5928 - val_loss: 0.1535 - val_acc: 0.9635 - val_auc: 0.5928\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 0.15341\n",
      "Epoch 275/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1544 - acc: 0.9635 - auc: 0.5929 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5929\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 0.15341\n",
      "Epoch 276/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1546 - acc: 0.9635 - auc: 0.5929 - val_loss: 0.1535 - val_acc: 0.9635 - val_auc: 0.5929\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 0.15341\n",
      "Epoch 277/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1545 - acc: 0.9635 - auc: 0.5929 - val_loss: 0.1535 - val_acc: 0.9635 - val_auc: 0.5929\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 0.15341\n",
      "Epoch 278/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1545 - acc: 0.9635 - auc: 0.5930 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5930\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 0.15341\n",
      "Epoch 279/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1545 - acc: 0.9635 - auc: 0.5930 - val_loss: 0.1535 - val_acc: 0.9635 - val_auc: 0.5930\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 0.15341\n",
      "Epoch 280/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1546 - acc: 0.9635 - auc: 0.5930 - val_loss: 0.1535 - val_acc: 0.9635 - val_auc: 0.5930\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 0.15341\n",
      "Epoch 281/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1544 - acc: 0.9635 - auc: 0.5931 - val_loss: 0.1535 - val_acc: 0.9635 - val_auc: 0.5931\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 0.15341\n",
      "Epoch 282/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1546 - acc: 0.9635 - auc: 0.5931 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5931\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 0.15341\n",
      "Epoch 283/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1546 - acc: 0.9635 - auc: 0.5931 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5931\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 0.15341\n",
      "Epoch 284/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1545 - acc: 0.9635 - auc: 0.5931 - val_loss: 0.1535 - val_acc: 0.9635 - val_auc: 0.5932\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 0.15341\n",
      "Epoch 285/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1546 - acc: 0.9635 - auc: 0.5932 - val_loss: 0.1535 - val_acc: 0.9635 - val_auc: 0.5932\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 0.15341\n",
      "Epoch 286/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1544 - acc: 0.9635 - auc: 0.5932 - val_loss: 0.1535 - val_acc: 0.9635 - val_auc: 0.5932\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 0.15341\n",
      "Epoch 287/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1546 - acc: 0.9635 - auc: 0.5933 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5933\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 0.15341\n",
      "Epoch 288/1000\n",
      "291698/291698 [==============================] - 35s 118us/step - loss: 0.1545 - acc: 0.9635 - auc: 0.5933 - val_loss: 0.1535 - val_acc: 0.9635 - val_auc: 0.5933\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 0.15341\n",
      "Epoch 289/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1545 - acc: 0.9635 - auc: 0.5933 - val_loss: 0.1535 - val_acc: 0.9635 - val_auc: 0.5933\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 0.15341\n",
      "Epoch 290/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1545 - acc: 0.9635 - auc: 0.5933 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5934\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 0.15341\n",
      "Epoch 291/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1545 - acc: 0.9635 - auc: 0.5934 - val_loss: 0.1535 - val_acc: 0.9635 - val_auc: 0.5934\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 0.15341\n",
      "Epoch 292/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1545 - acc: 0.9635 - auc: 0.5934 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5934\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 0.15341\n",
      "Epoch 293/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1545 - acc: 0.9635 - auc: 0.5934 - val_loss: 0.1535 - val_acc: 0.9635 - val_auc: 0.5934\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 0.15341\n",
      "Epoch 294/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1546 - acc: 0.9635 - auc: 0.5935 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5935\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 0.15341\n",
      "Epoch 295/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1547 - acc: 0.9635 - auc: 0.5935 - val_loss: 0.1535 - val_acc: 0.9635 - val_auc: 0.5935\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 0.15341\n",
      "Epoch 296/1000\n",
      "291698/291698 [==============================] - 35s 119us/step - loss: 0.1546 - acc: 0.9635 - auc: 0.5935 - val_loss: 0.1536 - val_acc: 0.9635 - val_auc: 0.5935\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 0.15341\n",
      "Epoch 00296: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, epochs=1000, batch_size=128, validation_data=(x_test, y_test),callbacks= [es,mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>car_01_cat</th>\n",
       "      <th>car_02_cat</th>\n",
       "      <th>car_03_cat</th>\n",
       "      <th>car_04_cat</th>\n",
       "      <th>car_05</th>\n",
       "      <th>car_06</th>\n",
       "      <th>car_07_cat</th>\n",
       "      <th>car_08_cat</th>\n",
       "      <th>car_09</th>\n",
       "      <th>car_10</th>\n",
       "      <th>...</th>\n",
       "      <th>idx_14</th>\n",
       "      <th>idx_15_bin</th>\n",
       "      <th>idx_16_bin</th>\n",
       "      <th>idx_17</th>\n",
       "      <th>idx_18</th>\n",
       "      <th>idx_19</th>\n",
       "      <th>idx_20</th>\n",
       "      <th>loc_01</th>\n",
       "      <th>loc_02</th>\n",
       "      <th>loc_03</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>3.464102</td>\n",
       "      <td>0.310322</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.776608</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.917878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>3.162278</td>\n",
       "      <td>0.385487</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.773609</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.387894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2.236068</td>\n",
       "      <td>0.412916</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.683271</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.562917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>3.605551</td>\n",
       "      <td>0.374566</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.783481</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.406202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>104</td>\n",
       "      <td>3.464102</td>\n",
       "      <td>0.539444</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.280802</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.784219</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   car_01_cat  car_02_cat  car_03_cat  car_04_cat    car_05    car_06  \\\n",
       "0           0           1           0          22  3.464102  0.310322   \n",
       "1           1           0           0          31  3.162278  0.385487   \n",
       "2          11          -1           0           7  2.236068  0.412916   \n",
       "3           0          -1           0          22  3.605551  0.374566   \n",
       "4           3           1           8         104  3.464102  0.539444   \n",
       "\n",
       "   car_07_cat  car_08_cat    car_09  car_10  ...  idx_14  idx_15_bin  \\\n",
       "0           1           1  0.776608       3  ...      11           0   \n",
       "1           1           1  0.773609       3  ...       9           0   \n",
       "2           1           1  0.683271       3  ...      11           0   \n",
       "3           1           1  0.783481       3  ...      10           0   \n",
       "4           1           1  1.280802       2  ...       9           0   \n",
       "\n",
       "   idx_16_bin  idx_17  idx_18  idx_19  idx_20  loc_01  loc_02    loc_03  \n",
       "0           0       1       1       9       3     0.3     0.7  0.917878  \n",
       "1           1       0       3       4       1     1.4     0.8  1.387894  \n",
       "2           0       2       2      12       5     0.2     0.6  0.562917  \n",
       "3           0       0       2       6       4     0.1     0.9  0.406202  \n",
       "4           0       3       2       7       3     0.0     0.3  0.784219  \n",
       "\n",
       "[5 rows x 57 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sample = pd.read_csv('test_sample.csv')\n",
    "test_sample = test_sample.drop('id', 1)\n",
    "test_target = test_sample.target\n",
    "test_sample = test_sample.drop('target', 1)\n",
    "test_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>car_01_cat</th>\n",
       "      <td>35700.0</td>\n",
       "      <td>0.387937</td>\n",
       "      <td>0.323792</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>car_02_cat</th>\n",
       "      <td>35700.0</td>\n",
       "      <td>0.422801</td>\n",
       "      <td>0.422409</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>car_03_cat</th>\n",
       "      <td>35700.0</td>\n",
       "      <td>0.081967</td>\n",
       "      <td>0.242369</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>car_04_cat</th>\n",
       "      <td>35700.0</td>\n",
       "      <td>0.596229</td>\n",
       "      <td>0.319499</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.300971</td>\n",
       "      <td>0.621359</td>\n",
       "      <td>0.893204</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>car_05</th>\n",
       "      <td>35700.0</td>\n",
       "      <td>0.819094</td>\n",
       "      <td>0.195691</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.755929</td>\n",
       "      <td>0.886405</td>\n",
       "      <td>0.963624</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>car_06</th>\n",
       "      <td>35700.0</td>\n",
       "      <td>0.779737</td>\n",
       "      <td>0.218867</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.814238</td>\n",
       "      <td>0.836461</td>\n",
       "      <td>0.853852</td>\n",
       "      <td>0.972630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>car_07_cat</th>\n",
       "      <td>35700.0</td>\n",
       "      <td>0.915560</td>\n",
       "      <td>0.187325</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>car_08_cat</th>\n",
       "      <td>35700.0</td>\n",
       "      <td>0.830644</td>\n",
       "      <td>0.375071</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>car_09</th>\n",
       "      <td>35700.0</td>\n",
       "      <td>0.162352</td>\n",
       "      <td>0.065099</td>\n",
       "      <td>0.023456</td>\n",
       "      <td>0.121113</td>\n",
       "      <td>0.148577</td>\n",
       "      <td>0.188771</td>\n",
       "      <td>0.848599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>car_10</th>\n",
       "      <td>35700.0</td>\n",
       "      <td>0.837374</td>\n",
       "      <td>0.207699</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>car_11</th>\n",
       "      <td>35700.0</td>\n",
       "      <td>0.609356</td>\n",
       "      <td>0.025663</td>\n",
       "      <td>0.485670</td>\n",
       "      <td>0.581139</td>\n",
       "      <td>0.606720</td>\n",
       "      <td>0.618126</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>car_12_cat</th>\n",
       "      <td>35700.0</td>\n",
       "      <td>0.466101</td>\n",
       "      <td>0.195594</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>car_13_cat</th>\n",
       "      <td>35700.0</td>\n",
       "      <td>0.776085</td>\n",
       "      <td>0.208248</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>car_14_cat</th>\n",
       "      <td>35700.0</td>\n",
       "      <td>0.249286</td>\n",
       "      <td>0.395329</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>car_15_cat</th>\n",
       "      <td>35700.0</td>\n",
       "      <td>0.496232</td>\n",
       "      <td>0.044358</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>car_16_cat</th>\n",
       "      <td>35700.0</td>\n",
       "      <td>0.955420</td>\n",
       "      <td>0.173393</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cus_01_bin</th>\n",
       "      <td>35700.0</td>\n",
       "      <td>0.391737</td>\n",
       "      <td>0.488145</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cus_02</th>\n",
       "      <td>35700.0</td>\n",
       "      <td>0.400968</td>\n",
       "      <td>0.244052</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cus_03_bin</th>\n",
       "      <td>35700.0</td>\n",
       "      <td>0.009692</td>\n",
       "      <td>0.097970</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cus_04_bin</th>\n",
       "      <td>35700.0</td>\n",
       "      <td>0.000420</td>\n",
       "      <td>0.020494</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cus_05</th>\n",
       "      <td>35700.0</td>\n",
       "      <td>0.003186</td>\n",
       "      <td>0.031844</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cus_06_bin</th>\n",
       "      <td>35700.0</td>\n",
       "      <td>0.154286</td>\n",
       "      <td>0.361227</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cus_07_bin</th>\n",
       "      <td>35700.0</td>\n",
       "      <td>0.184762</td>\n",
       "      <td>0.388110</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cus_08_bin</th>\n",
       "      <td>35700.0</td>\n",
       "      <td>0.163838</td>\n",
       "      <td>0.370133</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cus_09</th>\n",
       "      <td>35700.0</td>\n",
       "      <td>0.561023</td>\n",
       "      <td>0.271277</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cus_10_cat</th>\n",
       "      <td>35700.0</td>\n",
       "      <td>0.709090</td>\n",
       "      <td>0.246662</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cus_11_bin</th>\n",
       "      <td>35700.0</td>\n",
       "      <td>0.121429</td>\n",
       "      <td>0.326629</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cus_12_bin</th>\n",
       "      <td>35700.0</td>\n",
       "      <td>0.001541</td>\n",
       "      <td>0.039221</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cus_13</th>\n",
       "      <td>35700.0</td>\n",
       "      <td>0.271321</td>\n",
       "      <td>0.283457</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cus_14_cat</th>\n",
       "      <td>35700.0</td>\n",
       "      <td>0.201120</td>\n",
       "      <td>0.193311</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cus_15_bin</th>\n",
       "      <td>35700.0</td>\n",
       "      <td>0.660840</td>\n",
       "      <td>0.473431</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cus_16_cat</th>\n",
       "      <td>35700.0</td>\n",
       "      <td>0.472527</td>\n",
       "      <td>0.132461</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cus_17_bin</th>\n",
       "      <td>35700.0</td>\n",
       "      <td>0.001092</td>\n",
       "      <td>0.033034</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cus_18_bin</th>\n",
       "      <td>35700.0</td>\n",
       "      <td>0.259664</td>\n",
       "      <td>0.438456</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idx_01_bin</th>\n",
       "      <td>35700.0</td>\n",
       "      <td>0.553221</td>\n",
       "      <td>0.497166</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idx_02</th>\n",
       "      <td>35700.0</td>\n",
       "      <td>0.313431</td>\n",
       "      <td>0.190118</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idx_03</th>\n",
       "      <td>35700.0</td>\n",
       "      <td>0.499066</td>\n",
       "      <td>0.316899</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idx_04</th>\n",
       "      <td>35700.0</td>\n",
       "      <td>0.500364</td>\n",
       "      <td>0.318963</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idx_05_bin</th>\n",
       "      <td>35700.0</td>\n",
       "      <td>0.629188</td>\n",
       "      <td>0.483029</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idx_06_bin</th>\n",
       "      <td>35700.0</td>\n",
       "      <td>0.121877</td>\n",
       "      <td>0.327148</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idx_07</th>\n",
       "      <td>35700.0</td>\n",
       "      <td>0.334946</td>\n",
       "      <td>0.178858</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idx_08</th>\n",
       "      <td>35700.0</td>\n",
       "      <td>0.342851</td>\n",
       "      <td>0.124949</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.954545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idx_09</th>\n",
       "      <td>35700.0</td>\n",
       "      <td>0.472633</td>\n",
       "      <td>0.223914</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idx_10</th>\n",
       "      <td>35700.0</td>\n",
       "      <td>0.495994</td>\n",
       "      <td>0.319113</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idx_11</th>\n",
       "      <td>35700.0</td>\n",
       "      <td>0.301222</td>\n",
       "      <td>0.129120</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idx_12</th>\n",
       "      <td>35700.0</td>\n",
       "      <td>0.768258</td>\n",
       "      <td>0.133466</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idx_13_bin</th>\n",
       "      <td>35700.0</td>\n",
       "      <td>0.287367</td>\n",
       "      <td>0.452541</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idx_14</th>\n",
       "      <td>35700.0</td>\n",
       "      <td>0.722793</td>\n",
       "      <td>0.146758</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idx_15_bin</th>\n",
       "      <td>35700.0</td>\n",
       "      <td>0.348711</td>\n",
       "      <td>0.476569</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idx_16_bin</th>\n",
       "      <td>35700.0</td>\n",
       "      <td>0.152941</td>\n",
       "      <td>0.359936</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idx_17</th>\n",
       "      <td>35700.0</td>\n",
       "      <td>0.145361</td>\n",
       "      <td>0.121413</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idx_18</th>\n",
       "      <td>35700.0</td>\n",
       "      <td>0.221136</td>\n",
       "      <td>0.131155</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idx_19</th>\n",
       "      <td>35700.0</td>\n",
       "      <td>0.350642</td>\n",
       "      <td>0.120647</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.958333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idx_20</th>\n",
       "      <td>35700.0</td>\n",
       "      <td>0.334774</td>\n",
       "      <td>0.157343</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loc_01</th>\n",
       "      <td>35700.0</td>\n",
       "      <td>0.244364</td>\n",
       "      <td>0.224813</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loc_02</th>\n",
       "      <td>35700.0</td>\n",
       "      <td>0.678715</td>\n",
       "      <td>0.319121</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loc_03</th>\n",
       "      <td>35700.0</td>\n",
       "      <td>0.308282</td>\n",
       "      <td>0.157424</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.302821</td>\n",
       "      <td>0.342230</td>\n",
       "      <td>0.397607</td>\n",
       "      <td>0.891394</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              count      mean       std       min       25%       50%  \\\n",
       "car_01_cat  35700.0  0.387937  0.323792  0.000000  0.058824  0.411765   \n",
       "car_02_cat  35700.0  0.422801  0.422409  0.000000  0.000000  0.500000   \n",
       "car_03_cat  35700.0  0.081967  0.242369  0.000000  0.000000  0.000000   \n",
       "car_04_cat  35700.0  0.596229  0.319499  0.000000  0.300971  0.621359   \n",
       "car_05      35700.0  0.819094  0.195691  0.000000  0.755929  0.886405   \n",
       "car_06      35700.0  0.779737  0.218867  0.000000  0.814238  0.836461   \n",
       "car_07_cat  35700.0  0.915560  0.187325  0.500000  1.000000  1.000000   \n",
       "car_08_cat  35700.0  0.830644  0.375071  0.000000  1.000000  1.000000   \n",
       "car_09      35700.0  0.162352  0.065099  0.023456  0.121113  0.148577   \n",
       "car_10      35700.0  0.837374  0.207699  0.250000  0.750000  1.000000   \n",
       "car_11      35700.0  0.609356  0.025663  0.485670  0.581139  0.606720   \n",
       "car_12_cat  35700.0  0.466101  0.195594  0.000000  0.200000  0.600000   \n",
       "car_13_cat  35700.0  0.776085  0.208248  0.000000  0.666667  0.666667   \n",
       "car_14_cat  35700.0  0.249286  0.395329  0.000000  0.000000  0.000000   \n",
       "car_15_cat  35700.0  0.496232  0.044358  0.000000  0.500000  0.500000   \n",
       "car_16_cat  35700.0  0.955420  0.173393  0.000000  1.000000  1.000000   \n",
       "cus_01_bin  35700.0  0.391737  0.488145  0.000000  0.000000  0.000000   \n",
       "cus_02      35700.0  0.400968  0.244052  0.000000  0.181818  0.363636   \n",
       "cus_03_bin  35700.0  0.009692  0.097970  0.000000  0.000000  0.000000   \n",
       "cus_04_bin  35700.0  0.000420  0.020494  0.000000  0.000000  0.000000   \n",
       "cus_05      35700.0  0.003186  0.031844  0.000000  0.000000  0.000000   \n",
       "cus_06_bin  35700.0  0.154286  0.361227  0.000000  0.000000  0.000000   \n",
       "cus_07_bin  35700.0  0.184762  0.388110  0.000000  0.000000  0.000000   \n",
       "cus_08_bin  35700.0  0.163838  0.370133  0.000000  0.000000  0.000000   \n",
       "cus_09      35700.0  0.561023  0.271277  0.000000  0.384615  0.538462   \n",
       "cus_10_cat  35700.0  0.709090  0.246662  0.000000  0.500000  0.500000   \n",
       "cus_11_bin  35700.0  0.121429  0.326629  0.000000  0.000000  0.000000   \n",
       "cus_12_bin  35700.0  0.001541  0.039221  0.000000  0.000000  0.000000   \n",
       "cus_13      35700.0  0.271321  0.283457  0.000000  0.000000  0.142857   \n",
       "cus_14_cat  35700.0  0.201120  0.193311  0.000000  0.142857  0.142857   \n",
       "cus_15_bin  35700.0  0.660840  0.473431  0.000000  0.000000  1.000000   \n",
       "cus_16_cat  35700.0  0.472527  0.132461  0.000000  0.400000  0.400000   \n",
       "cus_17_bin  35700.0  0.001092  0.033034  0.000000  0.000000  0.000000   \n",
       "cus_18_bin  35700.0  0.259664  0.438456  0.000000  0.000000  0.000000   \n",
       "idx_01_bin  35700.0  0.553221  0.497166  0.000000  0.000000  1.000000   \n",
       "idx_02      35700.0  0.313431  0.190118  0.000000  0.166667  0.333333   \n",
       "idx_03      35700.0  0.499066  0.316899  0.000000  0.222222  0.555556   \n",
       "idx_04      35700.0  0.500364  0.318963  0.000000  0.222222  0.555556   \n",
       "idx_05_bin  35700.0  0.629188  0.483029  0.000000  0.000000  1.000000   \n",
       "idx_06_bin  35700.0  0.121877  0.327148  0.000000  0.000000  0.000000   \n",
       "idx_07      35700.0  0.334946  0.178858  0.000000  0.142857  0.285714   \n",
       "idx_08      35700.0  0.342851  0.124949  0.000000  0.272727  0.318182   \n",
       "idx_09      35700.0  0.472633  0.223914  0.000000  0.400000  0.400000   \n",
       "idx_10      35700.0  0.495994  0.319113  0.000000  0.222222  0.444444   \n",
       "idx_11      35700.0  0.301222  0.129120  0.000000  0.222222  0.277778   \n",
       "idx_12      35700.0  0.768258  0.133466  0.100000  0.700000  0.800000   \n",
       "idx_13_bin  35700.0  0.287367  0.452541  0.000000  0.000000  0.000000   \n",
       "idx_14      35700.0  0.722793  0.146758  0.000000  0.600000  0.700000   \n",
       "idx_15_bin  35700.0  0.348711  0.476569  0.000000  0.000000  0.000000   \n",
       "idx_16_bin  35700.0  0.152941  0.359936  0.000000  0.000000  0.000000   \n",
       "idx_17      35700.0  0.145361  0.121413  0.000000  0.100000  0.100000   \n",
       "idx_18      35700.0  0.221136  0.131155  0.000000  0.153846  0.230769   \n",
       "idx_19      35700.0  0.350642  0.120647  0.000000  0.250000  0.333333   \n",
       "idx_20      35700.0  0.334774  0.157343  0.000000  0.222222  0.333333   \n",
       "loc_01      35700.0  0.244364  0.224813  0.000000  0.111111  0.166667   \n",
       "loc_02      35700.0  0.678715  0.319121  0.000000  0.444444  0.777778   \n",
       "loc_03      35700.0  0.308282  0.157424  0.000000  0.302821  0.342230   \n",
       "\n",
       "                 75%       max  \n",
       "car_01_cat  0.647059  1.000000  \n",
       "car_02_cat  1.000000  1.000000  \n",
       "car_03_cat  0.000000  1.000000  \n",
       "car_04_cat  0.893204  1.000000  \n",
       "car_05      0.963624  1.000000  \n",
       "car_06      0.853852  0.972630  \n",
       "car_07_cat  1.000000  1.000000  \n",
       "car_08_cat  1.000000  1.000000  \n",
       "car_09      0.188771  0.848599  \n",
       "car_10      1.000000  1.000000  \n",
       "car_11      0.618126  1.000000  \n",
       "car_12_cat  0.600000  1.000000  \n",
       "car_13_cat  1.000000  1.000000  \n",
       "car_14_cat  0.500000  1.000000  \n",
       "car_15_cat  0.500000  1.000000  \n",
       "car_16_cat  1.000000  1.000000  \n",
       "cus_01_bin  1.000000  1.000000  \n",
       "cus_02      0.545455  1.000000  \n",
       "cus_03_bin  0.000000  1.000000  \n",
       "cus_04_bin  0.000000  1.000000  \n",
       "cus_05      0.000000  0.750000  \n",
       "cus_06_bin  0.000000  1.000000  \n",
       "cus_07_bin  0.000000  1.000000  \n",
       "cus_08_bin  0.000000  1.000000  \n",
       "cus_09      0.769231  1.000000  \n",
       "cus_10_cat  1.000000  1.000000  \n",
       "cus_11_bin  0.000000  1.000000  \n",
       "cus_12_bin  0.000000  1.000000  \n",
       "cus_13      0.428571  1.000000  \n",
       "cus_14_cat  0.142857  1.000000  \n",
       "cus_15_bin  1.000000  1.000000  \n",
       "cus_16_cat  0.600000  1.000000  \n",
       "cus_17_bin  0.000000  1.000000  \n",
       "cus_18_bin  1.000000  1.000000  \n",
       "idx_01_bin  1.000000  1.000000  \n",
       "idx_02      0.500000  1.000000  \n",
       "idx_03      0.777778  1.000000  \n",
       "idx_04      0.777778  1.000000  \n",
       "idx_05_bin  1.000000  1.000000  \n",
       "idx_06_bin  0.000000  1.000000  \n",
       "idx_07      0.428571  1.000000  \n",
       "idx_08      0.409091  0.954545  \n",
       "idx_09      0.600000  1.000000  \n",
       "idx_10      0.777778  1.000000  \n",
       "idx_11      0.388889  1.000000  \n",
       "idx_12      0.900000  1.000000  \n",
       "idx_13_bin  1.000000  1.000000  \n",
       "idx_14      0.800000  1.000000  \n",
       "idx_15_bin  1.000000  1.000000  \n",
       "idx_16_bin  0.000000  1.000000  \n",
       "idx_17      0.200000  0.900000  \n",
       "idx_18      0.307692  0.923077  \n",
       "idx_19      0.416667  0.958333  \n",
       "idx_20      0.444444  1.000000  \n",
       "loc_01      0.333333  1.000000  \n",
       "loc_02      1.000000  1.000000  \n",
       "loc_03      0.397607  0.891394  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sample = pd.DataFrame(scaler.transform(test_sample), columns=test_sample.columns)\n",
    "test_sample.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('best.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35700/35700 [==============================] - 2s 54us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.03615873],\n",
       "       [0.03648287],\n",
       "       [0.02653242],\n",
       "       ...,\n",
       "       [0.03625369],\n",
       "       [0.05722796],\n",
       "       [0.02683969]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model.predict(test_sample, verbose =1)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6333282647584974"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(test_target, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "epochs = range(1, 297)\n",
    "model_loss = history.history['val_loss']\n",
    "\n",
    "plt.plot(epochs, model_loss, 'ro', label='Dropout model')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T000000</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T000001</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T000002</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T000003</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T000004</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  target\n",
       "0  T000000     0.5\n",
       "1  T000001     0.5\n",
       "2  T000002     0.5\n",
       "3  T000003     0.5\n",
       "4  T000004     0.5"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sam = pd.read_csv('sample_submission.csv')\n",
    "sam.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = pd.read_csv('test_full.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.04032084],\n",
       "       [0.05808754],\n",
       "       [0.03597642],\n",
       "       ...,\n",
       "       [0.02837532],\n",
       "       [0.02934437],\n",
       "       [0.04133671]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = test_set.id\n",
    "test_set = test_set.drop('id', axis='columns')\n",
    "test_set = pd.DataFrame(scaler.transform(test_set), columns=test_set.columns)\n",
    "pred = model.predict(test_set)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T000000</td>\n",
       "      <td>0.040321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T000001</td>\n",
       "      <td>0.058088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T000002</td>\n",
       "      <td>0.035976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T000003</td>\n",
       "      <td>0.020864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T000004</td>\n",
       "      <td>0.027709</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id    target\n",
       "0  T000000  0.040321\n",
       "1  T000001  0.058088\n",
       "2  T000002  0.035976\n",
       "3  T000003  0.020864\n",
       "4  T000004  0.027709"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.DataFrame({'id':idx.values, 'target': pred.flatten()})\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('제출01.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
